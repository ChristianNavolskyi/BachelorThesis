Automatically generated by Mendeley Desktop 1.18
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Guo2005b,
abstract = {We describe our method for benchmarking Semantic Web knowledge base systems with respect to use in large OWL applications. We present the Lehigh University Benchmark (LUBM) as an example of how to design such benchmarks. The LUBM features an ontology for the university domain, synthetic OWL data scalable to an arbitrary size, 14 extensional queries representing a variety of properties, and several performance metrics. The LUBM can be used to evaluate systems with different reasoning capabilities and storage mechanisms. We demonstrate this with an evaluation of two memory-based systems and two systems with persistent storage. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Guo, Yuanbo and Pan, Zhengxiang and Heflin, Jeff},
doi = {10.1016/j.websem.2005.06.005},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Guo, Pan, Heflin - 2005 - LUBM A benchmark for OWL knowledge base systems.pdf:pdf},
isbn = {1570-8268},
issn = {15708268},
journal = {Web Semant.},
keywords = {Evaluation,Knowledge base system,Lehigh University Benchmark,Semantic Web},
number = {2-3},
pages = {158--182},
title = {{LUBM: A benchmark for OWL knowledge base systems}},
url = {http://www.websemanticsjournal.org/index.php/ps/article/download/70/68},
volume = {3},
year = {2005}
}
@article{Jordan2016a,
author = {Jordan, Matthias},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Jordan - 2016 - Benchmarking von Graphdatenbanken.pdf:pdf},
title = {{Benchmarking von Graphdatenbanken}},
year = {2016}
}
@article{Angles2014b,
abstract = {The Linked Data Benchmark Council (LDBC) is an EU project that aims to develop industry-strength benchmarks for graph and RDF data management systems. It in-cludes the creation of a non-profit LDBC organization, where industry players and academia come together for managing the development of benchmarks as well as auditing and publishing official results. We present an overview of the project including its goals and organi-zation, and describe its process and design methodology for benchmark development. We introduce so-called " choke-point " based benchmark development through which experts identify key technical challenges, and in-troduce them in the benchmark workload. Finally, we present the status of two benchmarks currently in devel-opment, one targeting graph data management systems using a social network data case, and the other targeting RDF systems using a data publishing case.},
author = {Angles, Renzo and Boncz, Peter and Larriba-Pey, Josep and Fundulaki, Irini and Neumann, Thomas and Erling, Orri and Neubauer, Peter and Martinez-Bazan, Norbert and Kotsev, Venelin and Toma, Ioan},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Angles et al. - 2014 - The Linked Data Benchmark Council a Graph and RDF industry benchmarking effort.pdf:pdf},
journal = {SIGMOD Rec.},
number = {1},
pages = {27--31},
title = {{The Linked Data Benchmark Council: a Graph and RDF industry benchmarking effort}},
url = {http://renzoangles.net/files/sigmodrecord2014.pdf},
volume = {43},
year = {2014}
}
@article{Capota2015b,
abstract = {Graphs are increasingly used in industry, governance, and science. This has stimulated the appearance of many and diverse graph-processing platforms. Although platform di- versity is beneficial, it alsomakes it very challenging to select the best platform for an application domain or one of its im- portant applications, and to design new and tune existing platforms. Continuing a long tradition of using benchmark- ing to address such challenges, in this work we present our vision for Graphalytics, a big data benchmark for graph- processing platforms. We have already benchmarked with Graphalytics a variety of popular platforms, such as Giraph, GraphX, and Neo4j.},
author = {Capotă, Mihai and Hegeman, Tim and Iosup, Alexandru and Prat-P{\'{e}}rez, Arnau and Erling, Orri and Boncz, Peter},
doi = {10.1145/2764947.2764954},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Capotă et al. - 2015 - Graphalytics.pdf:pdf},
isbn = {9781450336116},
journal = {Proc. GRADES'15 - GRADES'15},
pages = {1--6},
title = {{Graphalytics}},
url = {http://dl.acm.org/citation.cfm?doid=2764947.2764954},
year = {2015}
}
@article{TaoShena,
author = {{Tao Shen}, Heng and Pei, Jian and {Tamer {\"{O}}zsu}, M and Zou, Lei and Lu, Jiaheng and Ling, Tok-Wang and Yu, Ge and Zhuang, Yi and Shao, Jie},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Tao Shen et al. - Unknown - LNCS 6185 - Web-Age Information Management.pdf:pdf},
title = {{LNCS 6185 - Web-Age Information Management}},
url = {https://link.springer.com/content/pdf/10.1007/978-3-642-16720-1.pdf{\#}page=53}
}
@article{Worsch2011,
author = {Worsch, Thomas},
file = {:Users/navolskyi/Google Drive/Studium/Bachelor/Informatik/Theoretische Informatik/1. Grudbegriffe der Informatik/Folien/Skript/skript-2013.pdf:pdf},
number = {November},
title = {{Grundbegriffe der Informatik}},
url = {https://physik.leech.it/pub/Info/Grundbegriffe{\_}der{\_}Informatik/Skripte/WS{\_}10-11{\_}Skript{\_}Worsch.pdf},
year = {2011}
}
@article{Morsey2011b,
abstract = {Triple stores are the backbone of increasingly many Data Web appli-cations. It is thus evident that the performance of those stores is mission critical for individual projects as well as for data integration on the Data Web in gen-eral. Consequently, it is of central importance during the implementation of any of these applications to have a clear picture of the weaknesses and strengths of current triple store implementations. In this paper, we propose a generic SPARQL benchmark creation procedure, which we apply to the DBpedia knowledge base. Previous approaches often compared relational and triple stores and, thus, settled on measuring performance against a relational database which had been con-verted to RDF by using SQL-like queries. In contrast to those approaches, our benchmark is based on queries that were actually issued by humans and applica-tions against existing RDF data not resembling a relational schema. Our generic procedure for benchmark creation is based on query-log mining, clustering and SPARQL feature analysis. We argue that a pure SPARQL benchmark is more use-ful to compare existing triple stores and provide results for the popular triple store implementations Virtuoso, Sesame, Jena-TDB, and BigOWLIM. The subsequent comparison of our results with other benchmark results indicates that the per-formance of triple stores is by far less homogeneous than suggested by previous benchmarks.},
author = {Morsey, Mohamed and Lehmann, Jens and Auer, S{\"{o}}ren and {Ngonga Ngomo}, Axel-Cyrille},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Morsey et al. - 2011 - LNCS 7031 - DBpedia SPARQL Benchmark – Performance Assessment with Real Queries on Real Data.pdf:pdf},
title = {{LNCS 7031 - DBpedia SPARQL Benchmark – Performance Assessment with Real Queries on Real Data}},
url = {https://link.springer.com/content/pdf/10.1007/978-3-642-25073-6{\_}29.pdf},
year = {2011}
}
@article{Schmidt2008a,
abstract = {— Recently, the SPARQL query language for RDF has reached the W3C recommendation status. In response to this emerging standard, the database community is currently exploring efficient storage techniques for RDF data and evalua-tion strategies for SPARQL queries. A meaningful analysis and comparison of these approaches necessitates a comprehensive and universal benchmark platform. To this end, we have developed SP 2 Bench, a publicly available, language-specific SPARQL per-formance benchmark. SP 2 Bench is settled in the DBLP scenario and comprises both a data generator for creating arbitrarily large DBLP-like documents and a set of carefully designed benchmark queries. The generated documents mirror key characteristics and social-world distributions encountered in the original DBLP data set, while the queries implement meaningful requests on top of this data, covering a variety of SPARQL operator constellations and RDF access patterns. As a proof of concept, we apply SP 2 Bench to existing engines and discuss their strengths and weaknesses that follow immediately from the benchmark results.},
archivePrefix = {arXiv},
arxivId = {arXiv:0806.4627v2},
author = {Schmidt, Michael and Hornung, Thomas and Lausen, Georg and Pinkel, Christoph},
eprint = {arXiv:0806.4627v2},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Schmidt et al. - 2008 - Bench A SPARQL Performance Benchmark.pdf:pdf},
title = {{Bench: A SPARQL Performance Benchmark}},
url = {https://arxiv.org/pdf/0806.4627.pdf},
year = {2008}
}
@techreport{Iosupa,
abstract = {In this paper we introduce LDBC Graphalytics, a new industrial-grade benchmark for
graph analysis platforms. It consists of six deterministic algorithms, standard datasets,
synthetic dataset generators, and reference output, that enable the objective comparison
of graph analysis platforms. Its test harness produces deep metrics that quantify multiple
kinds of system scalability, such as horizontal/vertical and weak/strong, and of robustness,
such as failures and performance variability. The benchmark comes with opensource
software for generating data and monitoring performance. We describe and analyze
six implementations of the benchmark (three from the community, three from the
industry), providing insights into the strengths and weaknesses of the platforms. Key
to our contribution, vendors perform the tuning and benchmarking of their platforms.},
author = {Iosup, Alexandru and Hegeman, Tim and Ngai, Wing Lung and Heldens, Stijn and P{\'{e}}rez, Arnau Prat and Manhardt, Thomas and Chafi, Hassan and Capot˘, Mihai and Sundaram, Narayanan and Anderson, Michael and Gabriel, Ilie and Anase, T˘ and Xia, Yinglong and Nai, Lifeng and Boncz, Peter},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Iosup et al. - Unknown - LDBC Graphalytics A Benchmark for Large-Scale Graph Analysis on Parallel and Distributed Platforms, a Techni(2).pdf:pdf},
institution = {Delft University of Technology},
title = {{LDBC Graphalytics: A Benchmark for Large-Scale Graph Analysis on Parallel and Distributed Platforms, a Technical Report}},
url = {http://www.ds.ewi.tudelft.nl/fileadmin/pds/reports/2016/DS-2016-001.pdf}
}
@article{Dayarathna2012b,
abstract = {In recent years many online graph database service providers have started their operations in public clouds due to the growing demand for graph data storage and analysis. While this trend continues to rise there is little support available for benchmarking graph database systems in cloud environments. In this paper we introduce XGDBench which is a graph database benchmarking platformfor cloud computing systems. XGDBench has been designed to operate in current cloud service infras- tructures as well as on future exascale clouds. We extend the famous Yahoo! Cloud Serving Benchmark to the domain of graph database benchmarking. The benchmark platform is written in X10 which is a PGAS language intended for programming future exascale systems. We describe the architecture of the XGDBench focusing on its importance for exascale clouds.We did a cluster analysis to compare the realistic nature of XGDBench data generator and saw that the community structures of synthetic graphs produced by XGDBench outperforms RMAT. We also conducted performance evaluation of four famous graph data stores AllegroGraph, Fuseki, Neo4j, an OrientDB using XGDBench on Tsubame 2.0 HPC cloud environment.},
author = {Dayarathna, Miyuru and Suzumura, Toyotaro},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Dayarathna, Suzumura - 2012 - XGDBench A Bench m arking Platfor m for Graph Stores in Exascale Clouds.pdf:pdf},
isbn = {978-1-4673-4510-1/12},
keywords = {Benchmark testing,Database systems,Network the-,Performance analysis,System performance,ory (graphs)},
pages = {3--10},
title = {{XGDBench : A Bench m arking Platfor m for Graph Stores in Exascale Clouds}},
year = {2012}
}

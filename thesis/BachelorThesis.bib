Automatically generated by Mendeley Desktop 1.18
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Kasireddy2017a,
author = {Kasireddy, Preethi},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Kasireddy - 2017 - A Beginner-Friendly Introduction to Containers , VMs and Docker What are “ containers ” and “ VMs ”.pdf:pdf},
pages = {1--16},
title = {{A Beginner-Friendly Introduction to Containers , VMs and Docker What are “ containers ” and “ VMs ”?}},
year = {2017}
}
@article{Burns2016a,
abstract = {In the late 1980s and early 1990s, object-oriented pro- gramming revolutionized software development, popu- larizing the approach of building of applications as col- lections of modular components. Today we are seeing a similar revolution in distributed system development, with the increasing popularity of microservice archi- tectures built from containerized software components. Containers [15] [22] [1] [2] are particularly well-suited as the fundamental “object” in distributed systems by virtue of the walls they erect at the container bound- ary. As this architectural style matures, we are seeing the emergence of design patterns, much as we did for object- oriented programs, and for the same reason – thinking in terms of objects (or containers) abstracts away the low- level details of code, eventually revealing higher-level patterns that are common to a variety of applications and algorithms. This paper describes three types of design patterns that we have observed emerging in container-based dis- tributed systems: single-container patterns for container management, single-node patterns of closely cooperat- ing containers, and multi-node patterns for distributed algorithms. Like object-oriented patterns before them, these patterns for distributed computation encode best practices, simplify development, and make the systems where they are used more reliable.},
author = {Burns, Brendan},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Burns - 2016 - Design patterns for container-based distributed systems.pdf:pdf},
journal = {USENIX Work. Hot Top. Cloud Comput.},
title = {{Design patterns for container-based distributed systems}},
year = {2016}
}
@misc{Yegulalp2017,
author = {Yegulalp, Serdar},
title = {{What is NoSQL? NoSQL databases explained | InfoWorld}},
url = {https://www.infoworld.com/article/3240644/nosql/what-is-nosql-nosql-databases-explained.html},
urldate = {2018-04-28},
year = {2017}
}
@article{Capota2015c,
abstract = {Graphs are increasingly used in industry, governance, and science. This has stimulated the appearance of many and diverse graph-processing platforms. Although platform di- versity is beneficial, it alsomakes it very challenging to select the best platform for an application domain or one of its im- portant applications, and to design new and tune existing platforms. Continuing a long tradition of using benchmark- ing to address such challenges, in this work we present our vision for Graphalytics, a big data benchmark for graph- processing platforms. We have already benchmarked with Graphalytics a variety of popular platforms, such as Giraph, GraphX, and Neo4j.},
author = {Capotă, Mihai and Hegeman, Tim and Iosup, Alexandru and Prat-P{\'{e}}rez, Arnau and Erling, Orri and Boncz, Peter},
doi = {10.1145/2764947.2764954},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Capotă et al. - 2015 - Graphalytics.pdf:pdf},
isbn = {9781450336116},
journal = {Proc. GRADES'15 - GRADES'15},
pages = {1--6},
title = {{Graphalytics}},
url = {http://dl.acm.org/citation.cfm?doid=2764947.2764954},
year = {2015}
}
@techreport{Iosupa,
abstract = {In this paper we introduce LDBC Graphalytics, a new industrial-grade benchmark for
graph analysis platforms. It consists of six deterministic algorithms, standard datasets,
synthetic dataset generators, and reference output, that enable the objective comparison
of graph analysis platforms. Its test harness produces deep metrics that quantify multiple
kinds of system scalability, such as horizontal/vertical and weak/strong, and of robustness,
such as failures and performance variability. The benchmark comes with opensource
software for generating data and monitoring performance. We describe and analyze
six implementations of the benchmark (three from the community, three from the
industry), providing insights into the strengths and weaknesses of the platforms. Key
to our contribution, vendors perform the tuning and benchmarking of their platforms.},
author = {Iosup, Alexandru and Hegeman, Tim and Ngai, Wing Lung and Heldens, Stijn and P{\'{e}}rez, Arnau Prat and Manhardt, Thomas and Chafi, Hassan and Capot˘, Mihai and Sundaram, Narayanan and Anderson, Michael and Gabriel, Ilie and Anase, T˘ and Xia, Yinglong and Nai, Lifeng and Boncz, Peter},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Iosup et al. - Unknown - LDBC Graphalytics A Benchmark for Large-Scale Graph Analysis on Parallel and Distributed Platforms, a Techni(2).pdf:pdf},
institution = {Delft University of Technology},
title = {{LDBC Graphalytics: A Benchmark for Large-Scale Graph Analysis on Parallel and Distributed Platforms, a Technical Report}},
url = {http://www.ds.ewi.tudelft.nl/fileadmin/pds/reports/2016/DS-2016-001.pdf}
}
@article{Schmidt2008a,
abstract = {— Recently, the SPARQL query language for RDF has reached the W3C recommendation status. In response to this emerging standard, the database community is currently exploring efficient storage techniques for RDF data and evalua-tion strategies for SPARQL queries. A meaningful analysis and comparison of these approaches necessitates a comprehensive and universal benchmark platform. To this end, we have developed SP 2 Bench, a publicly available, language-specific SPARQL per-formance benchmark. SP 2 Bench is settled in the DBLP scenario and comprises both a data generator for creating arbitrarily large DBLP-like documents and a set of carefully designed benchmark queries. The generated documents mirror key characteristics and social-world distributions encountered in the original DBLP data set, while the queries implement meaningful requests on top of this data, covering a variety of SPARQL operator constellations and RDF access patterns. As a proof of concept, we apply SP 2 Bench to existing engines and discuss their strengths and weaknesses that follow immediately from the benchmark results.},
archivePrefix = {arXiv},
arxivId = {arXiv:0806.4627v2},
author = {Schmidt, Michael and Hornung, Thomas and Lausen, Georg and Pinkel, Christoph},
eprint = {arXiv:0806.4627v2},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Schmidt et al. - 2008 - Bench A SPARQL Performance Benchmark.pdf:pdf},
title = {{Bench: A SPARQL Performance Benchmark}},
url = {https://arxiv.org/pdf/0806.4627.pdf},
year = {2008}
}
@misc{Apache,
author = {Apache},
booktitle = {Apache Softw. Fund.},
title = {{Apache Jena - Apache Jena - TDB}},
url = {http://jena.apache.org/documentation/tdb/},
urldate = {2018-04-29}
}
@article{Jordan2016,
author = {Jordan, Matthias},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Jordan - 2016 - Benchmarking von Graphdatenbanken.pdf:pdf},
title = {{Benchmarking von Graphdatenbanken}},
year = {2016}
}
@article{Statement2017,
author = {Statement, Problem and Idea, Basic},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Statement, Idea - 2017 - Bachelor Thesis Drive By Maintenance.pdf:pdf},
pages = {6--8},
title = {{Bachelor Thesis ?: Drive By Maintenance ?}},
year = {2017}
}
@article{Angles2014a,
abstract = {The Linked Data Benchmark Council (LDBC) is an EU project that aims to develop industry-strength benchmarks for graph and RDF data management systems. It in-cludes the creation of a non-profit LDBC organization, where industry players and academia come together for managing the development of benchmarks as well as auditing and publishing official results. We present an overview of the project including its goals and organi-zation, and describe its process and design methodology for benchmark development. We introduce so-called " choke-point " based benchmark development through which experts identify key technical challenges, and in-troduce them in the benchmark workload. Finally, we present the status of two benchmarks currently in devel-opment, one targeting graph data management systems using a social network data case, and the other targeting RDF systems using a data publishing case.},
author = {Angles, Renzo and Boncz, Peter and Larriba-Pey, Josep and Fundulaki, Irini and Neumann, Thomas and Erling, Orri and Neubauer, Peter and Martinez-Bazan, Norbert and Kotsev, Venelin and Toma, Ioan},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Angles et al. - 2014 - The Linked Data Benchmark Council a Graph and RDF industry benchmarking effort.pdf:pdf},
journal = {SIGMOD Rec.},
number = {1},
pages = {27--31},
title = {{The Linked Data Benchmark Council: a Graph and RDF industry benchmarking effort}},
url = {http://renzoangles.net/files/sigmodrecord2014.pdf},
volume = {43},
year = {2014}
}
@article{TaoShen,
author = {{Tao Shen}, Heng and Pei, Jian and {Tamer {\"{O}}zsu}, M and Zou, Lei and Lu, Jiaheng and Ling, Tok-Wang and Yu, Ge and Zhuang, Yi and Shao, Jie},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Tao Shen et al. - Unknown - LNCS 6185 - Web-Age Information Management.pdf:pdf},
title = {{LNCS 6185 - Web-Age Information Management}},
url = {https://link.springer.com/content/pdf/10.1007/978-3-642-16720-1.pdf#page=53}
}
@article{Angles2014c,
abstract = {The Linked Data Benchmark Council (LDBC) is an EU project that aims to develop industry-strength benchmarks for graph and RDF data management systems. It in-cludes the creation of a non-profit LDBC organization, where industry players and academia come together for managing the development of benchmarks as well as auditing and publishing official results. We present an overview of the project including its goals and organi-zation, and describe its process and design methodology for benchmark development. We introduce so-called " choke-point " based benchmark development through which experts identify key technical challenges, and in-troduce them in the benchmark workload. Finally, we present the status of two benchmarks currently in devel-opment, one targeting graph data management systems using a social network data case, and the other targeting RDF systems using a data publishing case.},
author = {Angles, Renzo and Boncz, Peter and Larriba-Pey, Josep and Fundulaki, Irini and Neumann, Thomas and Erling, Orri and Neubauer, Peter and Martinez-Bazan, Norbert and Kotsev, Venelin and Toma, Ioan},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Angles et al. - 2014 - The Linked Data Benchmark Council a Graph and RDF industry benchmarking effort.pdf:pdf},
journal = {SIGMOD Rec.},
number = {1},
pages = {27--31},
title = {{The Linked Data Benchmark Council: a Graph and RDF industry benchmarking effort}},
url = {http://renzoangles.net/files/sigmodrecord2014.pdf},
volume = {43},
year = {2014}
}
@misc{Yahoo!2010,
author = {Yahoo!},
title = {{Yahoo! Cloud Serving Benchmark (YCSB) Wiki}},
url = {https://research.yahoo.com/news/yahoo-cloud-serving-benchmark/?guccounter=1},
urldate = {2018-04-29},
year = {2010}
}
@misc{Schneider2014,
author = {Schneider, Sven},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Schneider - 2014 - Final Presentation.pptx:pptx},
pages = {1--7},
title = {{Final Presentation}},
year = {2014}
}
@article{Angles2014b,
abstract = {The Linked Data Benchmark Council (LDBC) is an EU project that aims to develop industry-strength benchmarks for graph and RDF data management systems. It in-cludes the creation of a non-profit LDBC organization, where industry players and academia come together for managing the development of benchmarks as well as auditing and publishing official results. We present an overview of the project including its goals and organi-zation, and describe its process and design methodology for benchmark development. We introduce so-called " choke-point " based benchmark development through which experts identify key technical challenges, and in-troduce them in the benchmark workload. Finally, we present the status of two benchmarks currently in devel-opment, one targeting graph data management systems using a social network data case, and the other targeting RDF systems using a data publishing case.},
author = {Angles, Renzo and Boncz, Peter and Larriba-Pey, Josep and Fundulaki, Irini and Neumann, Thomas and Erling, Orri and Neubauer, Peter and Martinez-Bazan, Norbert and Kotsev, Venelin and Toma, Ioan},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Angles et al. - 2014 - The Linked Data Benchmark Council a Graph and RDF industry benchmarking effort.pdf:pdf},
journal = {SIGMOD Rec.},
number = {1},
pages = {27--31},
title = {{The Linked Data Benchmark Council: a Graph and RDF industry benchmarking effort}},
url = {http://renzoangles.net/files/sigmodrecord2014.pdf},
volume = {43},
year = {2014}
}
@misc{Apache2015,
author = {Apache},
booktitle = {Apache Softw. Fund.},
title = {{Getting Started with Apache Jena}},
url = {https://jena.apache.org/getting_started/},
urldate = {2018-04-29},
year = {2015}
}
@article{Angles2014,
abstract = {The Linked Data Benchmark Council (LDBC) is an EU project that aims to develop industry-strength benchmarks for graph and RDF data management systems. It in-cludes the creation of a non-profit LDBC organization, where industry players and academia come together for managing the development of benchmarks as well as auditing and publishing official results. We present an overview of the project including its goals and organi-zation, and describe its process and design methodology for benchmark development. We introduce so-called " choke-point " based benchmark development through which experts identify key technical challenges, and in-troduce them in the benchmark workload. Finally, we present the status of two benchmarks currently in devel-opment, one targeting graph data management systems using a social network data case, and the other targeting RDF systems using a data publishing case.},
author = {Angles, Renzo and Boncz, Peter and Larriba-Pey, Josep and Fundulaki, Irini and Neumann, Thomas and Erling, Orri and Neubauer, Peter and Martinez-Bazan, Norbert and Kotsev, Venelin and Toma, Ioan},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Angles et al. - 2014 - The Linked Data Benchmark Council a Graph and RDF industry benchmarking effort.pdf:pdf},
journal = {SIGMOD Rec.},
number = {1},
pages = {27--31},
title = {{The Linked Data Benchmark Council: a Graph and RDF industry benchmarking effort}},
url = {http://renzoangles.net/files/sigmodrecord2014.pdf},
volume = {43},
year = {2014}
}
@article{Schmidt2008c,
abstract = {— Recently, the SPARQL query language for RDF has reached the W3C recommendation status. In response to this emerging standard, the database community is currently exploring efficient storage techniques for RDF data and evalua-tion strategies for SPARQL queries. A meaningful analysis and comparison of these approaches necessitates a comprehensive and universal benchmark platform. To this end, we have developed SP 2 Bench, a publicly available, language-specific SPARQL per-formance benchmark. SP 2 Bench is settled in the DBLP scenario and comprises both a data generator for creating arbitrarily large DBLP-like documents and a set of carefully designed benchmark queries. The generated documents mirror key characteristics and social-world distributions encountered in the original DBLP data set, while the queries implement meaningful requests on top of this data, covering a variety of SPARQL operator constellations and RDF access patterns. As a proof of concept, we apply SP 2 Bench to existing engines and discuss their strengths and weaknesses that follow immediately from the benchmark results.},
archivePrefix = {arXiv},
arxivId = {arXiv:0806.4627v2},
author = {Schmidt, Michael and Hornung, Thomas and Lausen, Georg and Pinkel, Christoph},
eprint = {arXiv:0806.4627v2},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Schmidt et al. - 2008 - Bench A SPARQL Performance Benchmark.pdf:pdf},
title = {{Bench: A SPARQL Performance Benchmark}},
url = {https://arxiv.org/pdf/0806.4627.pdf},
year = {2008}
}
@article{Capota2015,
abstract = {Graphs are increasingly used in industry, governance, and science. This has stimulated the appearance of many and diverse graph-processing platforms. Although platform di- versity is beneficial, it alsomakes it very challenging to select the best platform for an application domain or one of its im- portant applications, and to design new and tune existing platforms. Continuing a long tradition of using benchmark- ing to address such challenges, in this work we present our vision for Graphalytics, a big data benchmark for graph- processing platforms. We have already benchmarked with Graphalytics a variety of popular platforms, such as Giraph, GraphX, and Neo4j.},
author = {Capotă, Mihai and Hegeman, Tim and Iosup, Alexandru and Prat-P{\'{e}}rez, Arnau and Erling, Orri and Boncz, Peter},
doi = {10.1145/2764947.2764954},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Capotă et al. - 2015 - Graphalytics.pdf:pdf},
isbn = {9781450336116},
journal = {Proc. GRADES'15 - GRADES'15},
pages = {1--6},
title = {{Graphalytics}},
url = {http://dl.acm.org/citation.cfm?doid=2764947.2764954},
year = {2015}
}
@article{Theoharis2005a,
abstract = {In this paper we benchmark three popular database repre- sentations of RDF/S schemata and data: (a) a schema-aware (i.e., one ta- ble per RDF/S class or property) with explicit (ISA) or implicit (NOISA) storage of subsumption relationships, (b) a schema-oblivious (i.e., a sin- gle table with triples of the form ?subject-predicate-object?), using (ID) or not (URI) identifiers to represent resources and (c) a hybrid of the schema-aware and schema-oblivious representations (i.e., one table per RDF/S meta-class by distinguishing also the range type of properties). Furthermore, we benchmark two common approaches for evaluating tax- onomic queries either on-the-fly (ISA, NOISA, Hybrid), or by precomput- ing the transitive closure of subsumption relationships (MatView, URI, ID). The main conclusion drawn from our experiments is that the evalua- tion of taxonomic queries is most efficient over RDF/S stores utilizing the Hybrid and MatView representations. Of the rest, schema-aware represen- tations (ISA, NOISA) exhibit overall better performance than URI,which is superior to that of ID, which exhibits the overall worst performance.},
author = {Theoharis, Yannis and Christophides, Vassilis and Karvounarakis, Grigoris},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Theoharis, Christophides, Karvounarakis - 2005 - Benchmarking Database Representations of RDF S Stores.pdf:pdf},
journal = {Springer-Verlag Berlin Heidelb.},
pages = {685--701},
title = {{Benchmarking Database Representations of RDF / S Stores}},
year = {2005}
}
@article{Jordan2016c,
author = {Jordan, Matthias},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Jordan - 2016 - Benchmarking von Graphdatenbanken.pdf:pdf},
title = {{Benchmarking von Graphdatenbanken}},
year = {2016}
}
@article{Morsey2011a,
abstract = {Triple stores are the backbone of increasingly many Data Web appli-cations. It is thus evident that the performance of those stores is mission critical for individual projects as well as for data integration on the Data Web in gen-eral. Consequently, it is of central importance during the implementation of any of these applications to have a clear picture of the weaknesses and strengths of current triple store implementations. In this paper, we propose a generic SPARQL benchmark creation procedure, which we apply to the DBpedia knowledge base. Previous approaches often compared relational and triple stores and, thus, settled on measuring performance against a relational database which had been con-verted to RDF by using SQL-like queries. In contrast to those approaches, our benchmark is based on queries that were actually issued by humans and applica-tions against existing RDF data not resembling a relational schema. Our generic procedure for benchmark creation is based on query-log mining, clustering and SPARQL feature analysis. We argue that a pure SPARQL benchmark is more use-ful to compare existing triple stores and provide results for the popular triple store implementations Virtuoso, Sesame, Jena-TDB, and BigOWLIM. The subsequent comparison of our results with other benchmark results indicates that the per-formance of triple stores is by far less homogeneous than suggested by previous benchmarks.},
author = {Morsey, Mohamed and Lehmann, Jens and Auer, S{\"{o}}ren and {Ngonga Ngomo}, Axel-Cyrille},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Morsey et al. - 2011 - LNCS 7031 - DBpedia SPARQL Benchmark – Performance Assessment with Real Queries on Real Data.pdf:pdf},
title = {{LNCS 7031 - DBpedia SPARQL Benchmark – Performance Assessment with Real Queries on Real Data}},
url = {https://link.springer.com/content/pdf/10.1007/978-3-642-25073-6_29.pdf},
year = {2011}
}
@article{Morsey2011b,
abstract = {Triple stores are the backbone of increasingly many Data Web appli-cations. It is thus evident that the performance of those stores is mission critical for individual projects as well as for data integration on the Data Web in gen-eral. Consequently, it is of central importance during the implementation of any of these applications to have a clear picture of the weaknesses and strengths of current triple store implementations. In this paper, we propose a generic SPARQL benchmark creation procedure, which we apply to the DBpedia knowledge base. Previous approaches often compared relational and triple stores and, thus, settled on measuring performance against a relational database which had been con-verted to RDF by using SQL-like queries. In contrast to those approaches, our benchmark is based on queries that were actually issued by humans and applica-tions against existing RDF data not resembling a relational schema. Our generic procedure for benchmark creation is based on query-log mining, clustering and SPARQL feature analysis. We argue that a pure SPARQL benchmark is more use-ful to compare existing triple stores and provide results for the popular triple store implementations Virtuoso, Sesame, Jena-TDB, and BigOWLIM. The subsequent comparison of our results with other benchmark results indicates that the per-formance of triple stores is by far less homogeneous than suggested by previous benchmarks.},
author = {Morsey, Mohamed and Lehmann, Jens and Auer, S{\"{o}}ren and {Ngonga Ngomo}, Axel-Cyrille},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Morsey et al. - 2011 - LNCS 7031 - DBpedia SPARQL Benchmark – Performance Assessment with Real Queries on Real Data.pdf:pdf},
title = {{LNCS 7031 - DBpedia SPARQL Benchmark – Performance Assessment with Real Queries on Real Data}},
url = {https://link.springer.com/content/pdf/10.1007/978-3-642-25073-6_29.pdf},
year = {2011}
}
@article{Slater2015,
author = {Slater, Nate},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Slater - 2015 - Using Containers to Build a Microservices Architecture.pdf:pdf},
pages = {1--12},
title = {{Using Containers to Build a Microservices Architecture}},
url = {https://medium.com/aws-activate-startup-blog/using-containers-to-build-a-microservices-architecture-6e1b8bacb7d1#.6myity7wu},
year = {2015}
}
@article{Phippya,
author = {Phippy, Introducing},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Phippy - Unknown - The Children ' s Illustrated Guide to Kubernetes The Other Day ... The Children ' s Illustrated Guide to Kubernetes.pdf:pdf},
title = {{The Children ' s Illustrated Guide to Kubernetes The Other Day ... The Children ' s Illustrated Guide to Kubernetes}}
}
@article{Capota2015a,
abstract = {Graphs are increasingly used in industry, governance, and science. This has stimulated the appearance of many and diverse graph-processing platforms. Although platform di- versity is beneficial, it alsomakes it very challenging to select the best platform for an application domain or one of its im- portant applications, and to design new and tune existing platforms. Continuing a long tradition of using benchmark- ing to address such challenges, in this work we present our vision for Graphalytics, a big data benchmark for graph- processing platforms. We have already benchmarked with Graphalytics a variety of popular platforms, such as Giraph, GraphX, and Neo4j.},
author = {Capotă, Mihai and Hegeman, Tim and Iosup, Alexandru and Prat-P{\'{e}}rez, Arnau and Erling, Orri and Boncz, Peter},
doi = {10.1145/2764947.2764954},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Capotă et al. - 2015 - Graphalytics.pdf:pdf},
isbn = {9781450336116},
journal = {Proc. GRADES'15 - GRADES'15},
pages = {1--6},
title = {{Graphalytics}},
url = {http://dl.acm.org/citation.cfm?doid=2764947.2764954},
year = {2015}
}
@article{Worsch2011,
author = {Worsch, Thomas},
file = {:Users/navolskyi/Google Drive/Studium/Bachelor/Informatik/Theoretische Informatik/1. Grudbegriffe der Informatik/Folien/Skript/skript-2013.pdf:pdf},
number = {November},
title = {{Grundbegriffe der Informatik}},
url = {https://physik.leech.it/pub/Info/Grundbegriffe_der_Informatik/Skripte/WS_10-11_Skript_Worsch.pdf},
year = {2011}
}
@misc{Techopedia2017,
author = {Techopedia},
title = {{What is a Data Dictionary? - Definition from Techopedia}},
url = {https://www.techopedia.com/definition/30329/document-oriented-database},
urldate = {2018-04-29},
year = {2017}
}
@article{Angles2012,
abstract = {The limitations of traditional databases, in particular the relational model, to cover the requirements of current applications has lead the development of new database technologies. Among them, the Graph Databases are calling the attention of the database community because in trendy projects where a database is needed, the extraction of worthy information relies on processing the graph-like structure of the data. In this paper we present a systematic comparison of current graph database models. Our review includes general features (for data storing and querying), data modeling features (i.e., data structures, query languages, and integrity constraints), and the support for essential graph queries.},
author = {Angles, Renzo},
doi = {10.1109/ICDEW.2012.31},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Angles - 2012 - A comparison of current graph database models.pdf:pdf},
isbn = {9780769547480},
journal = {Proc. - 2012 IEEE 28th Int. Conf. Data Eng. Work. ICDEW 2012},
pages = {171--177},
title = {{A comparison of current graph database models}},
year = {2012}
}
@article{Morsey2011c,
abstract = {Triple stores are the backbone of increasingly many Data Web appli-cations. It is thus evident that the performance of those stores is mission critical for individual projects as well as for data integration on the Data Web in gen-eral. Consequently, it is of central importance during the implementation of any of these applications to have a clear picture of the weaknesses and strengths of current triple store implementations. In this paper, we propose a generic SPARQL benchmark creation procedure, which we apply to the DBpedia knowledge base. Previous approaches often compared relational and triple stores and, thus, settled on measuring performance against a relational database which had been con-verted to RDF by using SQL-like queries. In contrast to those approaches, our benchmark is based on queries that were actually issued by humans and applica-tions against existing RDF data not resembling a relational schema. Our generic procedure for benchmark creation is based on query-log mining, clustering and SPARQL feature analysis. We argue that a pure SPARQL benchmark is more use-ful to compare existing triple stores and provide results for the popular triple store implementations Virtuoso, Sesame, Jena-TDB, and BigOWLIM. The subsequent comparison of our results with other benchmark results indicates that the per-formance of triple stores is by far less homogeneous than suggested by previous benchmarks.},
author = {Morsey, Mohamed and Lehmann, Jens and Auer, S{\"{o}}ren and {Ngonga Ngomo}, Axel-Cyrille},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Morsey et al. - 2011 - LNCS 7031 - DBpedia SPARQL Benchmark – Performance Assessment with Real Queries on Real Data.pdf:pdf},
title = {{LNCS 7031 - DBpedia SPARQL Benchmark – Performance Assessment with Real Queries on Real Data}},
url = {https://link.springer.com/content/pdf/10.1007/978-3-642-25073-6_29.pdf},
year = {2011}
}
@misc{W3C2014,
abstract = {In addition to the classic “Web of documents” W3C is helping to build a technology stack to support a “Web of data,” the sort of data you find in databases. The ultimate goal of the Web of data is to enable computers to do more useful work and to develop systems that can support trusted interactions over the network. The term “Semantic Web” refers to W3C's vision of the Web of linked data. Semantic Web technologies enable people to create data stores on the Web, build vocabularies, and write rules for handling data. Linked data are empowered by technologies such as RDF, SPARQL, JSON-LD, OWL, and SKOS.},
author = {W3C},
booktitle = {W3C - Semant. Web},
title = {{Semantic Web Standards}},
url = {https://www.w3.org/RDF/},
urldate = {2018-04-29},
year = {2014}
}
@article{TaoShena,
author = {{Tao Shen}, Heng and Pei, Jian and {Tamer {\"{O}}zsu}, M and Zou, Lei and Lu, Jiaheng and Ling, Tok-Wang and Yu, Ge and Zhuang, Yi and Shao, Jie},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Tao Shen et al. - Unknown - LNCS 6185 - Web-Age Information Management.pdf:pdf},
title = {{LNCS 6185 - Web-Age Information Management}},
url = {https://link.springer.com/content/pdf/10.1007/978-3-642-16720-1.pdf#page=53}
}
@article{Wu,
abstract = {—Modern cloud providers offer dense hardware with multiple cores and large memories, hosted in global platforms. This raises the challenge of implementing high-performance software systems that can effectively scale from a single core to multicore to the globe. Conventional wisdom says that software designed for one scale point needs to be rewritten when scaling up by 10−100× [1]. In contrast, we explore how a system can be architected to scale across many orders of magnitude by design. We explore this challenge in the context of a new key-value store system called Anna: a partitioned, multi-mastered system that achieves high performance and elasticity via wait-free execution and coordination-free consistency. Our design rests on a simple architecture of coordination-free actors that perform state update via merge of lattice-based composite data structures. We demonstrate that a wide variety of consistency models can be elegantly implemented in this architecture with unprecedented consistency, smooth fine-grained elasticity, and performance that far exceeds the state of the art.},
author = {Wu, Chenggang and Faleiro, Jose M and Lin, Yihan and Hellerstein, Joseph M},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - Unknown - Anna A KVS For Any Scale.pdf:pdf},
title = {{Anna: A KVS For Any Scale}},
url = {http://db.cs.berkeley.edu/jmh/papers/anna_ieee18.pdf}
}
@article{Schmidt2008b,
abstract = {— Recently, the SPARQL query language for RDF has reached the W3C recommendation status. In response to this emerging standard, the database community is currently exploring efficient storage techniques for RDF data and evalua-tion strategies for SPARQL queries. A meaningful analysis and comparison of these approaches necessitates a comprehensive and universal benchmark platform. To this end, we have developed SP 2 Bench, a publicly available, language-specific SPARQL per-formance benchmark. SP 2 Bench is settled in the DBLP scenario and comprises both a data generator for creating arbitrarily large DBLP-like documents and a set of carefully designed benchmark queries. The generated documents mirror key characteristics and social-world distributions encountered in the original DBLP data set, while the queries implement meaningful requests on top of this data, covering a variety of SPARQL operator constellations and RDF access patterns. As a proof of concept, we apply SP 2 Bench to existing engines and discuss their strengths and weaknesses that follow immediately from the benchmark results.},
archivePrefix = {arXiv},
arxivId = {arXiv:0806.4627v2},
author = {Schmidt, Michael and Hornung, Thomas and Lausen, Georg and Pinkel, Christoph},
eprint = {arXiv:0806.4627v2},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Schmidt et al. - 2008 - Bench A SPARQL Performance Benchmark.pdf:pdf},
title = {{Bench: A SPARQL Performance Benchmark}},
url = {https://arxiv.org/pdf/0806.4627.pdf},
year = {2008}
}
@techreport{Iosup,
abstract = {In this paper we introduce LDBC Graphalytics, a new industrial-grade benchmark for graph analysis platforms. It consists of six deterministic algorithms, standard datasets, synthetic dataset generators, and reference output, that enable the objective comparison of graph analysis platforms. Its test harness produces deep metrics that quantify multiple kinds of system scalability, such as horizontal/vertical and weak/strong, and of robustness, such as failures and performance variability. The benchmark comes with opensource software for generating data and monitoring performance. We describe and analyze six implementations of the benchmark (three from the community, three from the industry), providing insights into the strengths and weaknesses of the platforms. Key to our contribution, vendors perform the tuning and benchmarking of their platforms.},
author = {Iosup, Alexandru and Hegeman, Tim and Ngai, Wing Lung and Heldens, Stijn and P{\'{e}}rez, Arnau Prat and Manhardt, Thomas and Chafi, Hassan and Capot˘, Mihai and Sundaram, Narayanan and Anderson, Michael and Gabriel, Ilie and Anase, T˘ and Xia, Yinglong and Nai, Lifeng and Boncz, Peter},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Iosup et al. - Unknown - LDBC Graphalytics A Benchmark for Large-Scale Graph Analysis on Parallel and Distributed Platforms, a Techni(2).pdf:pdf},
institution = {Delft University of Technology},
title = {{LDBC Graphalytics: A Benchmark for Large-Scale Graph Analysis on Parallel and Distributed Platforms, a Technical Report}},
url = {http://www.ds.ewi.tudelft.nl/fileadmin/pds/reports/2016/DS-2016-001.pdf}
}
@article{Capota2015b,
abstract = {Graphs are increasingly used in industry, governance, and science. This has stimulated the appearance of many and diverse graph-processing platforms. Although platform di- versity is beneficial, it alsomakes it very challenging to select the best platform for an application domain or one of its im- portant applications, and to design new and tune existing platforms. Continuing a long tradition of using benchmark- ing to address such challenges, in this work we present our vision for Graphalytics, a big data benchmark for graph- processing platforms. We have already benchmarked with Graphalytics a variety of popular platforms, such as Giraph, GraphX, and Neo4j.},
author = {Capotă, Mihai and Hegeman, Tim and Iosup, Alexandru and Prat-P{\'{e}}rez, Arnau and Erling, Orri and Boncz, Peter},
doi = {10.1145/2764947.2764954},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Capotă et al. - 2015 - Graphalytics.pdf:pdf},
isbn = {9781450336116},
journal = {Proc. GRADES'15 - GRADES'15},
pages = {1--6},
title = {{Graphalytics}},
url = {http://dl.acm.org/citation.cfm?doid=2764947.2764954},
year = {2015}
}
@misc{Rouse2016,
author = {Rouse, Margaret},
title = {{What is data collection? - Definition from WhatIs.com}},
url = {https://whatis.techtarget.com/definition/graph-database},
urldate = {2018-04-28},
year = {2016}
}
@article{Jordan2016b,
author = {Jordan, Matthias},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Jordan - 2016 - Benchmarking von Graphdatenbanken.pdf:pdf},
title = {{Benchmarking von Graphdatenbanken}},
year = {2016}
}
@misc{AlaaMahmoud2017,
author = {{Alaa Mahmoud}},
title = {{An Overview of Graph Database Query Languages}},
url = {https://developer.ibm.com/dwblog/2017/overview-graph-database-query-languages/},
urldate = {2018-03-17},
year = {2017}
}
@article{Guo2005c,
abstract = {We describe our method for benchmarking Semantic Web knowledge base systems with respect to use in large OWL applications. We present the Lehigh University Benchmark (LUBM) as an example of how to design such benchmarks. The LUBM features an ontology for the university domain, synthetic OWL data scalable to an arbitrary size, 14 extensional queries representing a variety of properties, and several performance metrics. The LUBM can be used to evaluate systems with different reasoning capabilities and storage mechanisms. We demonstrate this with an evaluation of two memory-based systems and two systems with persistent storage. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Guo, Yuanbo and Pan, Zhengxiang and Heflin, Jeff},
doi = {10.1016/j.websem.2005.06.005},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Guo, Pan, Heflin - 2005 - LUBM A benchmark for OWL knowledge base systems.pdf:pdf},
isbn = {1570-8268},
issn = {15708268},
journal = {Web Semant.},
keywords = {Evaluation,Knowledge base system,Lehigh University Benchmark,Semantic Web},
number = {2-3},
pages = {158--182},
title = {{LUBM: A benchmark for OWL knowledge base systems}},
url = {http://www.websemanticsjournal.org/index.php/ps/article/download/70/68},
volume = {3},
year = {2005}
}
@article{Theoharis2005b,
abstract = {In this paper we benchmark three popular database repre- sentations of RDF/S schemata and data: (a) a schema-aware (i.e., one ta- ble per RDF/S class or property) with explicit (ISA) or implicit (NOISA) storage of subsumption relationships, (b) a schema-oblivious (i.e., a sin- gle table with triples of the form ?subject-predicate-object?), using (ID) or not (URI) identifiers to represent resources and (c) a hybrid of the schema-aware and schema-oblivious representations (i.e., one table per RDF/S meta-class by distinguishing also the range type of properties). Furthermore, we benchmark two common approaches for evaluating tax- onomic queries either on-the-fly (ISA, NOISA, Hybrid), or by precomput- ing the transitive closure of subsumption relationships (MatView, URI, ID). The main conclusion drawn from our experiments is that the evalua- tion of taxonomic queries is most efficient over RDF/S stores utilizing the Hybrid and MatView representations. Of the rest, schema-aware represen- tations (ISA, NOISA) exhibit overall better performance than URI,which is superior to that of ID, which exhibits the overall worst performance.},
author = {Theoharis, Yannis and Christophides, Vassilis and Karvounarakis, Grigoris},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Theoharis, Christophides, Karvounarakis - 2005 - Benchmarking Database Representations of RDF S Stores.pdf:pdf},
journal = {Springer-Verlag Berlin Heidelb.},
pages = {685--701},
title = {{Benchmarking Database Representations of RDF / S Stores}},
year = {2005}
}
@article{Jordan2016a,
author = {Jordan, Matthias},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Jordan - 2016 - Benchmarking von Graphdatenbanken.pdf:pdf},
title = {{Benchmarking von Graphdatenbanken}},
year = {2016}
}
@article{Dayarathna2012b,
abstract = {In recent years many online graph database service providers have started their operations in public clouds due to the growing demand for graph data storage and analysis. While this trend continues to rise there is little support available for benchmarking graph database systems in cloud environments. In this paper we introduce XGDBench which is a graph database benchmarking platformfor cloud computing systems. XGDBench has been designed to operate in current cloud service infras- tructures as well as on future exascale clouds. We extend the famous Yahoo! Cloud Serving Benchmark to the domain of graph database benchmarking. The benchmark platform is written in X10 which is a PGAS language intended for programming future exascale systems. We describe the architecture of the XGDBench focusing on its importance for exascale clouds.We did a cluster analysis to compare the realistic nature of XGDBench data generator and saw that the community structures of synthetic graphs produced by XGDBench outperforms RMAT. We also conducted performance evaluation of four famous graph data stores AllegroGraph, Fuseki, Neo4j, an OrientDB using XGDBench on Tsubame 2.0 HPC cloud environment.},
author = {Dayarathna, Miyuru and Suzumura, Toyotaro},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Dayarathna, Suzumura - 2012 - XGDBench A Bench m arking Platfor m for Graph Stores in Exascale Clouds.pdf:pdf},
isbn = {978-1-4673-4510-1/12},
keywords = {Benchmark testing,Database systems,Network the-,Performance analysis,System performance,ory (graphs)},
pages = {3--10},
title = {{XGDBench : A Bench m arking Platfor m for Graph Stores in Exascale Clouds}},
year = {2012}
}
@article{Ferdinand2016,
author = {Ferdinand, M},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Ferdinand - 2016 - Design of a Linked Data-enabled Microservice Platform for the Industrial Internet of Things.pdf:pdf},
title = {{Design of a Linked Data-enabled Microservice Platform for the Industrial Internet of Things}},
year = {2016}
}
@misc{Neo4jInc.,
abstract = {This chapter contains the complete and authoritative documentation for the Cypher query language.},
author = {{Neo4j Inc.}},
title = {{Chapter 3. Cypher - The Neo4j Developer Manual v3.3}},
url = {https://neo4j.com/docs/developer-manual/current/introduction/#introduction-highlights},
urldate = {2018-04-29}
}
@article{Curry2012,
abstract = {Within the operational phase buildings are now producing more data than ever before, from energy usage, utility information, occupancy patterns, weather data, etc. In order to manage a building holistically it is important to use knowledge from across these information sources. However, many barriers exist to their interoperability and there is little interaction between these islands of information. As part of moving building data to the cloud there is a critical need to reflect on the design of cloudbased data services and how they are designed from an interoperability perspective. If new cloud data services are designed in the same manner as traditional building management systems they will suffer from the data interoperability problems. Linked data technology leverages the existing open protocols and W3C standards of the Web architecture for sharing structured data on the web. In this paper we propose the use of linked data as an enabling technology for cloud-based building data services. The objective of linking building data in the cloud is to create an integrated well-connected graph of relevant information for managing a building. This paper describes the fundamentals of the approach and demonstrates the concept within a Small Medium sized Enterprise (SME) with an owner-occupied office building.},
author = {Curry, Edward and O'Donnell, James and Corry, Edward and Hasan, Souleiman and Keane, Marcus and O'Riain, Se{\'{a}}n},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Curry et al. - 2012 - Linking building data in the cloud Integrating cross-domain building data using linked data.pdf:pdf},
journal = {Elsevier Ltd.},
keywords = {Building energy analysis,Building infromation model,Building management,Data Interoperability,Data es a service,Linked data},
mendeley-tags = {Building energy analysis,Building infromation model,Building management,Data Interoperability,Data es a service,Linked data},
pages = {206--219},
title = {{Linking building data in the cloud: Integrating cross-domain building data using linked data}},
url = {http://s3.amazonaws.com/academia.edu.documents/41524058/Linking_building_data_in_the_cloud_Integ20160124-26435-1mfp5qv.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1499852371&Signature=wJPbGo7WnEgFytJCGI4kj3qeipk%253D&response-content-disposition=inlin},
year = {2012}
}
@article{Dayarathna2012a,
abstract = {In recent years many online graph database service providers have started their operations in public clouds due to the growing demand for graph data storage and analysis. While this trend continues to rise there is little support available for benchmarking graph database systems in cloud environments. In this paper we introduce XGDBench which is a graph database benchmarking platformfor cloud computing systems. XGDBench has been designed to operate in current cloud service infras- tructures as well as on future exascale clouds. We extend the famous Yahoo! Cloud Serving Benchmark to the domain of graph database benchmarking. The benchmark platform is written in X10 which is a PGAS language intended for programming future exascale systems. We describe the architecture of the XGDBench focusing on its importance for exascale clouds.We did a cluster analysis to compare the realistic nature of XGDBench data generator and saw that the community structures of synthetic graphs produced by XGDBench outperforms RMAT. We also conducted performance evaluation of four famous graph data stores AllegroGraph, Fuseki, Neo4j, an OrientDB using XGDBench on Tsubame 2.0 HPC cloud environment.},
author = {Dayarathna, Miyuru and Suzumura, Toyotaro},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Dayarathna, Suzumura - 2012 - XGDBench A Bench m arking Platfor m for Graph Stores in Exascale Clouds.pdf:pdf},
isbn = {978-1-4673-4510-1/12},
keywords = {Benchmark testing,Database systems,Network the-,Performance analysis,System performance,ory (graphs)},
pages = {3--10},
title = {{XGDBench : A Bench m arking Platfor m for Graph Stores in Exascale Clouds}},
year = {2012}
}
@misc{Ontotext2014,
author = {Ontotext},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Ontotext - 2014 - THE TRUTH ABOUT TRIPLESTORES(2).pdf:pdf},
howpublished = {https://ontotext.com/wp-content/uploads/2014/07/The-Truth-About-Triplestores.pdf},
pages = {28},
title = {{The Truth About Triplestores}},
url = {https://ontotext.com/wp-content/uploads/2014/07/The-Truth-About-Triplestores.pdf},
year = {2014}
}
@article{Guo2005b,
abstract = {We describe our method for benchmarking Semantic Web knowledge base systems with respect to use in large OWL applications. We present the Lehigh University Benchmark (LUBM) as an example of how to design such benchmarks. The LUBM features an ontology for the university domain, synthetic OWL data scalable to an arbitrary size, 14 extensional queries representing a variety of properties, and several performance metrics. The LUBM can be used to evaluate systems with different reasoning capabilities and storage mechanisms. We demonstrate this with an evaluation of two memory-based systems and two systems with persistent storage. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Guo, Yuanbo and Pan, Zhengxiang and Heflin, Jeff},
doi = {10.1016/j.websem.2005.06.005},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Guo, Pan, Heflin - 2005 - LUBM A benchmark for OWL knowledge base systems.pdf:pdf},
isbn = {1570-8268},
issn = {15708268},
journal = {Web Semant.},
keywords = {Evaluation,Knowledge base system,Lehigh University Benchmark,Semantic Web},
number = {2-3},
pages = {158--182},
title = {{LUBM: A benchmark for OWL knowledge base systems}},
url = {http://www.websemanticsjournal.org/index.php/ps/article/download/70/68},
volume = {3},
year = {2005}
}
@article{Curry2012b,
abstract = {Within the operational phase buildings are now producing more data than ever before, from energy usage, utility information, occupancy patterns, weather data, etc. In order to manage a building holistically it is important to use knowledge from across these information sources. However, many barriers exist to their interoperability and there is little interaction between these islands of information. As part of moving building data to the cloud there is a critical need to reflect on the design of cloudbased data services and how they are designed from an interoperability perspective. If new cloud data services are designed in the same manner as traditional building management systems they will suffer from the data interoperability problems. Linked data technology leverages the existing open protocols and W3C standards of the Web architecture for sharing structured data on the web. In this paper we propose the use of linked data as an enabling technology for cloud-based building data services. The objective of linking building data in the cloud is to create an integrated well-connected graph of relevant information for managing a building. This paper describes the fundamentals of the approach and demonstrates the concept within a Small Medium sized Enterprise (SME) with an owner-occupied office building.},
author = {Curry, Edward and O'Donnell, James and Corry, Edward and Hasan, Souleiman and Keane, Marcus and O'Riain, Se{\'{a}}n},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Curry et al. - 2012 - Linking building data in the cloud Integrating cross-domain building data using linked data.pdf:pdf},
journal = {Elsevier Ltd.},
keywords = {Building energy analysis,Building infromation model,Building management,Data Interoperability,Data es a service,Linked data},
mendeley-tags = {Building energy analysis,Building infromation model,Building management,Data Interoperability,Data es a service,Linked data},
pages = {206--219},
title = {{Linking building data in the cloud: Integrating cross-domain building data using linked data}},
url = {http://s3.amazonaws.com/academia.edu.documents/41524058/Linking_building_data_in_the_cloud_Integ20160124-26435-1mfp5qv.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1499852371&Signature=wJPbGo7WnEgFytJCGI4kj3qeipk%253D&response-content-disposition=inlin},
year = {2012}
}
@article{Danz,
author = {Danz, M and Gr{\"{a}}f, T and Michel, C},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Danz, Gr{\"{a}}f, Michel - Unknown - A structured approach to evidence-based software engineering in empirical software engineering research.pdf:pdf},
keywords = {checklist,ebse,evidence-based software engineering,flowchart,scientific working,software engineering,student workflow},
title = {{A structured approach to evidence-based software engineering in empirical software engineering research for students}}
}
@techreport{Haque2014,
abstract = {Abstract An RDF (Resource Description Framework) instance generator produces RDF triples by complying with an ontology that defines classes, subclasses, relations, and constraints. There aremany instance generatorswhich rely onWebOntology Language (OWL) meaning that these generators can read only the ontologies which are written in OWL. However, the existing generators are locked-in to a specific ontology, which means the generators can read only a specific ontology. For instance, the LUBM generator can read only the LUBM ontology, which is clearly a limitation as it is not able to read any other ontology such as biomodel ontology. This promotes the need for a generic RDF instance generator that is able to read and parse any ontology written in OWL. In this technical report, we describe a generic RDF instance generator. We de- velop this generator to enable users to use their own ontology to generate RDF triples which can be used to meet their specific needs. Keywords:},
author = {Haque, Rafiqul and Amir, Samir and Ait-Kaci, Hassan and Raynaud, Tanguy and Hacid, Mohand-Said},
file = {:Users/navolskyi/Downloads/ctr14.pdf:pdf},
keywords = {Instances generation,OWL,Ontology,RDF,SemanticWeb,Taxonomy},
number = {November},
title = {{Technical Report Number 14}},
year = {2014}
}
@article{Guo2005,
abstract = {We describe our method for benchmarking Semantic Web knowledge base systems with respect to use in large OWL applications. We present the Lehigh University Benchmark (LUBM) as an example of how to design such benchmarks. The LUBM features an ontology for the university domain, synthetic OWL data scalable to an arbitrary size, 14 extensional queries representing a variety of properties, and several performance metrics. The LUBM can be used to evaluate systems with different reasoning capabilities and storage mechanisms. We demonstrate this with an evaluation of two memory-based systems and two systems with persistent storage. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Guo, Yuanbo and Pan, Zhengxiang and Heflin, Jeff},
doi = {10.1016/j.websem.2005.06.005},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Guo, Pan, Heflin - 2005 - LUBM A benchmark for OWL knowledge base systems.pdf:pdf},
isbn = {1570-8268},
issn = {15708268},
journal = {Web Semant.},
keywords = {Evaluation,Knowledge base system,Lehigh University Benchmark,Semantic Web},
number = {2-3},
pages = {158--182},
title = {{LUBM: A benchmark for OWL knowledge base systems}},
url = {http://www.websemanticsjournal.org/index.php/ps/article/download/70/68},
volume = {3},
year = {2005}
}
@misc{NeoTechnologyInc.2016,
author = {{Neo Technology Inc.}},
title = {{Relational Databases vs. Graph Databases: A Comparison}},
url = {https://neo4j.com/developer/graph-db-vs-rdbms/#_from_relational_to_graph_databases},
urldate = {2018-04-29},
year = {2016}
}
@article{Burns2016b,
abstract = {In the late 1980s and early 1990s, object-oriented pro- gramming revolutionized software development, popu- larizing the approach of building of applications as col- lections of modular components. Today we are seeing a similar revolution in distributed system development, with the increasing popularity of microservice archi- tectures built from containerized software components. Containers [15] [22] [1] [2] are particularly well-suited as the fundamental “object” in distributed systems by virtue of the walls they erect at the container bound- ary. As this architectural style matures, we are seeing the emergence of design patterns, much as we did for object- oriented programs, and for the same reason – thinking in terms of objects (or containers) abstracts away the low- level details of code, eventually revealing higher-level patterns that are common to a variety of applications and algorithms. This paper describes three types of design patterns that we have observed emerging in container-based dis- tributed systems: single-container patterns for container management, single-node patterns of closely cooperat- ing containers, and multi-node patterns for distributed algorithms. Like object-oriented patterns before them, these patterns for distributed computation encode best practices, simplify development, and make the systems where they are used more reliable.},
author = {Burns, Brendan},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Burns - 2016 - Design patterns for container-based distributed systems.pdf:pdf},
journal = {USENIX Work. Hot Top. Cloud Comput.},
title = {{Design patterns for container-based distributed systems}},
year = {2016}
}
@techreport{Iosupc,
abstract = {In this paper we introduce LDBC Graphalytics, a new industrial-grade benchmark for
graph analysis platforms. It consists of six deterministic algorithms, standard datasets,
synthetic dataset generators, and reference output, that enable the objective comparison
of graph analysis platforms. Its test harness produces deep metrics that quantify multiple
kinds of system scalability, such as horizontal/vertical and weak/strong, and of robustness,
such as failures and performance variability. The benchmark comes with opensource
software for generating data and monitoring performance. We describe and analyze
six implementations of the benchmark (three from the community, three from the
industry), providing insights into the strengths and weaknesses of the platforms. Key
to our contribution, vendors perform the tuning and benchmarking of their platforms.},
author = {Iosup, Alexandru and Hegeman, Tim and Ngai, Wing Lung and Heldens, Stijn and P{\'{e}}rez, Arnau Prat and Manhardt, Thomas and Chafi, Hassan and Capot˘, Mihai and Sundaram, Narayanan and Anderson, Michael and Gabriel, Ilie and Anase, T˘ and Xia, Yinglong and Nai, Lifeng and Boncz, Peter},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Iosup et al. - Unknown - LDBC Graphalytics A Benchmark for Large-Scale Graph Analysis on Parallel and Distributed Platforms, a Techni(2).pdf:pdf},
institution = {Delft University of Technology},
title = {{LDBC Graphalytics: A Benchmark for Large-Scale Graph Analysis on Parallel and Distributed Platforms, a Technical Report}},
url = {http://www.ds.ewi.tudelft.nl/fileadmin/pds/reports/2016/DS-2016-001.pdf}
}
@article{Schmidt2008,
abstract = {— Recently, the SPARQL query language for RDF has reached the W3C recommendation status. In response to this emerging standard, the database community is currently exploring efficient storage techniques for RDF data and evalua-tion strategies for SPARQL queries. A meaningful analysis and comparison of these approaches necessitates a comprehensive and universal benchmark platform. To this end, we have developed SP 2 Bench, a publicly available, language-specific SPARQL per-formance benchmark. SP 2 Bench is settled in the DBLP scenario and comprises both a data generator for creating arbitrarily large DBLP-like documents and a set of carefully designed benchmark queries. The generated documents mirror key characteristics and social-world distributions encountered in the original DBLP data set, while the queries implement meaningful requests on top of this data, covering a variety of SPARQL operator constellations and RDF access patterns. As a proof of concept, we apply SP 2 Bench to existing engines and discuss their strengths and weaknesses that follow immediately from the benchmark results.},
archivePrefix = {arXiv},
arxivId = {arXiv:0806.4627v2},
author = {Schmidt, Michael and Hornung, Thomas and Lausen, Georg and Pinkel, Christoph},
eprint = {arXiv:0806.4627v2},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Schmidt et al. - 2008 - Bench A SPARQL Performance Benchmark.pdf:pdf},
title = {{Bench: A SPARQL Performance Benchmark}},
url = {https://arxiv.org/pdf/0806.4627.pdf},
year = {2008}
}
@article{Dayarathna2012,
abstract = {In recent years many online graph database service providers have started their operations in public clouds due to the growing demand for graph data storage and analysis. While this trend continues to rise there is little support available for benchmarking graph database systems in cloud environments. In this paper we introduce XGDBench which is a graph database benchmarking platformfor cloud computing systems. XGDBench has been designed to operate in current cloud service infras- tructures as well as on future exascale clouds. We extend the famous Yahoo! Cloud Serving Benchmark to the domain of graph database benchmarking. The benchmark platform is written in X10 which is a PGAS language intended for programming future exascale systems. We describe the architecture of the XGDBench focusing on its importance for exascale clouds.We did a cluster analysis to compare the realistic nature of XGDBench data generator and saw that the community structures of synthetic graphs produced by XGDBench outperforms RMAT. We also conducted performance evaluation of four famous graph data stores AllegroGraph, Fuseki, Neo4j, an OrientDB using XGDBench on Tsubame 2.0 HPC cloud environment.},
author = {Dayarathna, Miyuru and Suzumura, Toyotaro},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Dayarathna, Suzumura - 2012 - XGDBench A Bench m arking Platfor m for Graph Stores in Exascale Clouds.pdf:pdf},
isbn = {978-1-4673-4510-1/12},
keywords = {Benchmark testing,Database systems,Network the-,Performance analysis,System performance,ory (graphs)},
pages = {363--370},
title = {{XGDBench : A Bench m arking Platfor m for Graph Stores in Exascale Clouds}},
year = {2012}
}
@article{Phippy,
author = {Phippy, Introducing},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Phippy - Unknown - The Children ' s Illustrated Guide to Kubernetes The Other Day ... The Children ' s Illustrated Guide to Kubernetes.pdf:pdf},
title = {{The Children ' s Illustrated Guide to Kubernetes The Other Day ... The Children ' s Illustrated Guide to Kubernetes}}
}
@misc{Ontotext2014,
author = {Ontotext},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Ontotext - 2014 - THE TRUTH ABOUT TRIPLESTORES(2).pdf:pdf},
pages = {28},
title = {{The Truth About Triplestores}},
url = {https://ontotext.com/wp-content/uploads/2014/07/The-Truth-About-Triplestores.pdf},
year = {2014}
}
@article{Theoharis2005,
abstract = {In this paper we benchmark three popular database repre- sentations of RDF/S schemata and data: (a) a schema-aware (i.e., one ta- ble per RDF/S class or property) with explicit (ISA) or implicit (NOISA) storage of subsumption relationships, (b) a schema-oblivious (i.e., a sin- gle table with triples of the form ?subject-predicate-object?), using (ID) or not (URI) identifiers to represent resources and (c) a hybrid of the schema-aware and schema-oblivious representations (i.e., one table per RDF/S meta-class by distinguishing also the range type of properties). Furthermore, we benchmark two common approaches for evaluating tax- onomic queries either on-the-fly (ISA, NOISA, Hybrid), or by precomput- ing the transitive closure of subsumption relationships (MatView, URI, ID). The main conclusion drawn from our experiments is that the evalua- tion of taxonomic queries is most efficient over RDF/S stores utilizing the Hybrid and MatView representations. Of the rest, schema-aware represen- tations (ISA, NOISA) exhibit overall better performance than URI,which is superior to that of ID, which exhibits the overall worst performance.},
author = {Theoharis, Yannis and Christophides, Vassilis and Karvounarakis, Grigoris},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Theoharis, Christophides, Karvounarakis - 2005 - Benchmarking Database Representations of RDF S Stores.pdf:pdf},
journal = {Springer-Verlag Berlin Heidelb.},
pages = {685--701},
title = {{Benchmarking Database Representations of RDF / S Stores}},
year = {2005}
}
@misc{ChuaHock-Chuan,
author = {Hock-Chuan, Chua},
keywords = {ER,database,dbms,mysql,relations},
pages = {1--8},
title = {{A Quick-Start Tutorial on Relational Database Design Database Design Objectiv e}},
url = {https://www.ntu.edu.sg/home/ehchua/programming/sql/Relational_Database_Design.html},
urldate = {2018-04-28}
}
@article{Burns2016,
abstract = {In the late 1980s and early 1990s, object-oriented pro- gramming revolutionized software development, popu- larizing the approach of building of applications as col- lections of modular components. Today we are seeing a similar revolution in distributed system development, with the increasing popularity of microservice archi- tectures built from containerized software components. Containers [15] [22] [1] [2] are particularly well-suited as the fundamental “object” in distributed systems by virtue of the walls they erect at the container bound- ary. As this architectural style matures, we are seeing the emergence of design patterns, much as we did for object- oriented programs, and for the same reason – thinking in terms of objects (or containers) abstracts away the low- level details of code, eventually revealing higher-level patterns that are common to a variety of applications and algorithms. This paper describes three types of design patterns that we have observed emerging in container-based dis- tributed systems: single-container patterns for container management, single-node patterns of closely cooperat- ing containers, and multi-node patterns for distributed algorithms. Like object-oriented patterns before them, these patterns for distributed computation encode best practices, simplify development, and make the systems where they are used more reliable.},
author = {Burns, Brendan},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Burns - 2016 - Design patterns for container-based distributed systems.pdf:pdf},
journal = {USENIX Work. Hot Top. Cloud Comput.},
title = {{Design patterns for container-based distributed systems}},
year = {2016}
}
@article{Aggarwal2011,
abstract = {There is much research on social network analysis but only recently did scholars turn their attention to the volatility of social networks. An abundance of questions emerged. How does a social network evolve – can we find laws and derive models that explain its evolution? How do communities emerge in a social network and how do they expand or shrink? What is a community in an evolving network – can we claim that two communities seen at two distinct timepoints are the same one, even if they have next to no members in common? Research advances have different perspectives: some scholars focus on how evolution manifests itself in a social network, while others investigate how individual communities evolve as new members join and old ones become inactive. There are methods for discovering communities and capturing their changes in time, and methods that consider a community as a smoothly evolving constellation and thus build and adapt models upon that premise. This survey organizes advances on evolution in social networks into a common framework and gives an overview of these different perspectives.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Aggarwal, Charu},
doi = {10.1007/978-1-4419-8462-3},
eprint = {arXiv:1011.1669v3},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Aggarwal - 2011 - Social Network Data Analytics.pdf:pdf},
isbn = {978-1-4419-8461-6},
issn = {05333164},
journal = {Vasa},
keywords = {aggarwal,c,components of an entire,ed,global scale,how do the different,how do they evolve,look like on a,network form,over time,social network data analytics,what,what do social networks},
pages = {499},
pmid = {15003161},
title = {{Social Network Data Analytics}},
url = {http://medcontent.metapress.com/index/A65RM03P4874243N.pdf http://www.springer.com/cda/content/document/cda_downloaddocument/9781441984616-c2.pdf?SGWID=0-0-45-1100037-p174095463},
year = {2011}
}
@article{Slater2015a,
author = {Slater, Nate},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Slater - 2015 - Using Containers to Build a Microservices Architecture.pdf:pdf},
pages = {1--12},
title = {{Using Containers to Build a Microservices Architecture}},
url = {https://medium.com/aws-activate-startup-blog/using-containers-to-build-a-microservices-architecture-6e1b8bacb7d1#.6myity7wu},
year = {2015}
}
@techreport{SparsityTechnologies,
abstract = {This is the complete User Manual for Sparksee graph database which covers all the technical details relating to Sparksee, so it should be the go-to document for any doubts regarding this graph database.},
author = {{Sparsity Technologies}},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Sparsity Technologies - Unknown - User Manual - Sparksee(3).pdf:pdf},
pages = {147},
title = {{User Manual - Sparksee}},
url = {http://www.sparsity-technologies.com/downloads/UserManual.pdf}
}
@misc{Apache2016,
author = {Apache},
booktitle = {W3C Recomm. 25 June 2013},
title = {{Apache Jena - Fuseki serving RDF data over HTTP}},
url = {https://jena.apache.org/documentation/serving_data/},
urldate = {2018-04-29},
year = {2016}
}
@techreport{Iosupb,
abstract = {In this paper we introduce LDBC Graphalytics, a new industrial-grade benchmark for
graph analysis platforms. It consists of six deterministic algorithms, standard datasets,
synthetic dataset generators, and reference output, that enable the objective comparison
of graph analysis platforms. Its test harness produces deep metrics that quantify multiple
kinds of system scalability, such as horizontal/vertical and weak/strong, and of robustness,
such as failures and performance variability. The benchmark comes with opensource
software for generating data and monitoring performance. We describe and analyze
six implementations of the benchmark (three from the community, three from the
industry), providing insights into the strengths and weaknesses of the platforms. Key
to our contribution, vendors perform the tuning and benchmarking of their platforms.},
author = {Iosup, Alexandru and Hegeman, Tim and Ngai, Wing Lung and Heldens, Stijn and P{\'{e}}rez, Arnau Prat and Manhardt, Thomas and Chafi, Hassan and Capot˘, Mihai and Sundaram, Narayanan and Anderson, Michael and Gabriel, Ilie and Anase, T˘ and Xia, Yinglong and Nai, Lifeng and Boncz, Peter},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Iosup et al. - Unknown - LDBC Graphalytics A Benchmark for Large-Scale Graph Analysis on Parallel and Distributed Platforms, a Techni(2).pdf:pdf},
institution = {Delft University of Technology},
title = {{LDBC Graphalytics: A Benchmark for Large-Scale Graph Analysis on Parallel and Distributed Platforms, a Technical Report}},
url = {http://www.ds.ewi.tudelft.nl/fileadmin/pds/reports/2016/DS-2016-001.pdf}
}
@article{Angles2012a,
abstract = {The limitations of traditional databases, in particular the relational model, to cover the requirements of current applications has lead the development of new database technologies. Among them, the Graph Databases are calling the attention of the database community because in trendy projects where a database is needed, the extraction of worthy information relies on processing the graph-like structure of the data. In this paper we present a systematic comparison of current graph database models. Our review includes general features (for data storing and querying), data modeling features (i.e., data structures, query languages, and integrity constraints), and the support for essential graph queries.},
author = {Angles, Renzo},
doi = {10.1109/ICDEW.2012.31},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Angles - 2012 - A comparison of current graph database models.pdf:pdf},
isbn = {9780769547480},
journal = {Proc. - 2012 IEEE 28th Int. Conf. Data Eng. Work. ICDEW 2012},
pages = {171--177},
title = {{A comparison of current graph database models}},
year = {2012}
}
@techreport{Bader2009,
abstract = {Contributors: David A. Bader (Georgia Tech), John Feo (Cray), John Gilbert (UC Santa Barbara), Jeremy Kepner (MIT/LL), David Koester (Mitre), Eugene Loh (Sun Microsystems), Kamesh Madduri (Georgia Tech), Bill Mann (formerly of MIT/LL), Theresa Meuse (MIT/LL), Eric Robinson (MIT/LL) Version History: V1.0: Released 24 February 2009 Version 1.0 of this document was generated primarily based on the HPCS SSCA#2 benchmark version 2.2. It was modified to present a more general graph analysis benchmark to the high performance computing (HPC) community. 2.0 Brief Description of the Scalable Graph Analysis Benchmark The intent of this benchmark is to develop a compact application that has multiple analysis techniques (multiple kernels) accessing a single data structure representing a weighted, directed graph. In addition to a kernel to construct the graph from the input tuple list, there will be three additional computational kernels to operate on the graph. Each of the kernels will require irregular access to the graph's data structure, and it is possible that no single data layout will be optimal for all four computational kernels. This benchmark includes a scalable data generator that produces edge tuples containing the start vertex, end vertex, and weight for each directed edge. The first kernel constructs the graph in a format usable by all subsequent kernels. No subsequent modifications are permitted to benefit specific kernels. The second kernel extracts edges by weight from the graph representation and forms a list of the selected edges. The third kernel extracts a series of subgraphs formed by following paths of specified length from a start set of initial vertices. The set of initial vertices are determined by kernel 2. The fourth computational kernel computes a centrality metric that identifies vertices of key importance along shortest paths of the graph. All the kernels are timed.},
author = {Bader, David A and Feo, John and Gilbert, John and Kepner, Jeremy and Koester, David and Loh, Eugene and Madduri, Kamesh and Mann, Bill and Meuse, Theresa and Robinson, Eric},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Bader et al. - 2009 - HPC Scalable Graph Analysis Benchmark.pdf:pdf},
title = {{HPC Scalable Graph Analysis Benchmark}},
url = {http://www.graphanalysis.org/benchmark/GraphAnalysisBenchmark-v1.0.pdf},
year = {2009}
}
@article{Dayarathna2012,
abstract = {In recent years many online graph database service providers have started their operations in public clouds due to the growing demand for graph data storage and analysis. While this trend continues to rise there is little support available for benchmarking graph database systems in cloud environments. In this paper we introduce XGDBench which is a graph database benchmarking platformfor cloud computing systems. XGDBench has been designed to operate in current cloud service infras- tructures as well as on future exascale clouds. We extend the famous Yahoo! Cloud Serving Benchmark to the domain of graph database benchmarking. The benchmark platform is written in X10 which is a PGAS language intended for programming future exascale systems. We describe the architecture of the XGDBench focusing on its importance for exascale clouds.We did a cluster analysis to compare the realistic nature of XGDBench data generator and saw that the community structures of synthetic graphs produced by XGDBench outperforms RMAT. We also conducted performance evaluation of four famous graph data stores AllegroGraph, Fuseki, Neo4j, an OrientDB using XGDBench on Tsubame 2.0 HPC cloud environment.},
author = {Dayarathna, Miyuru and Suzumura, Toyotaro},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Dayarathna, Suzumura - 2012 - XGDBench A Bench m arking Platfor m for Graph Stores in Exascale Clouds.pdf:pdf},
isbn = {978-1-4673-4510-1/12},
keywords = {Benchmark testing,Database systems,Network the-,Performance analysis,System performance,ory (graphs)},
pages = {3--10},
title = {{XGDBench : A Bench m arking Platfor m for Graph Stores in Exascale Clouds}},
year = {2012}
}
@article{Morsey2011,
abstract = {Triple stores are the backbone of increasingly many Data Web appli-cations. It is thus evident that the performance of those stores is mission critical for individual projects as well as for data integration on the Data Web in gen-eral. Consequently, it is of central importance during the implementation of any of these applications to have a clear picture of the weaknesses and strengths of current triple store implementations. In this paper, we propose a generic SPARQL benchmark creation procedure, which we apply to the DBpedia knowledge base. Previous approaches often compared relational and triple stores and, thus, settled on measuring performance against a relational database which had been con-verted to RDF by using SQL-like queries. In contrast to those approaches, our benchmark is based on queries that were actually issued by humans and applica-tions against existing RDF data not resembling a relational schema. Our generic procedure for benchmark creation is based on query-log mining, clustering and SPARQL feature analysis. We argue that a pure SPARQL benchmark is more use-ful to compare existing triple stores and provide results for the popular triple store implementations Virtuoso, Sesame, Jena-TDB, and BigOWLIM. The subsequent comparison of our results with other benchmark results indicates that the per-formance of triple stores is by far less homogeneous than suggested by previous benchmarks.},
author = {Morsey, Mohamed and Lehmann, Jens and Auer, S{\"{o}}ren and {Ngonga Ngomo}, Axel-Cyrille},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Morsey et al. - 2011 - LNCS 7031 - DBpedia SPARQL Benchmark – Performance Assessment with Real Queries on Real Data.pdf:pdf},
title = {{LNCS 7031 - DBpedia SPARQL Benchmark – Performance Assessment with Real Queries on Real Data}},
url = {https://link.springer.com/content/pdf/10.1007/978-3-642-25073-6_29.pdf},
year = {2011}
}
@article{Farrugia2010,
author = {Farrugia, Patricia and Petrisor, Bradley A. and Farrokhyar, Forough and Bhandari, Mohit},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Farrugia et al. - 2010 - Research questions, hypotheses and objektives.pdf:pdf},
journal = {Can J Surg},
pages = {3},
title = {{Research questions, hypotheses and objektives}},
url = {https://isites.harvard.edu/fs/docs/icb.topic1172952.files/04082013.pdf},
volume = {53},
year = {2010}
}
@article{TaoShen,
author = {Dominguez-Sal, D. and Urb{\'{o}}n-Bayes, P. and Gim{\'{e}}nez-Va{\~{n}}{\'{o}}, A. and G{\'{o}}mez-Villamor, S. and Mart{\'{i}}nez-Baz{\'{a}}n, N. and Larriba-Pey, J.L.},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Tao Shen et al. - Unknown - LNCS 6185 - Web-Age Information Management.pdf:pdf},
pages = {37--48},
title = {{LNCS 6185 - Web-Age Information Management}},
url = {https://link.springer.com/content/pdf/10.1007/978-3-642-16720-1.pdf#page=53}
}
@article{TaoShenb,
author = {Dominguez-Sal, D. and Urb{\'{o}}n-Bayes, P. and Gim{\'{e}}nez-Va{\~{n}}{\'{o}}, A. and G{\'{o}}mez-Villamor, S. and Mart{\'{i}}nez-Baz{\'{a}}n, N. and Larriba-Pey, J.L.},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Tao Shen et al. - Unknown - LNCS 6185 - Web-Age Information Management.pdf:pdf;:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Tao Shen et al. - Unknown - LNCS 6185 - Web-Age Information Management(2).pdf:pdf},
pages = {37--48},
title = {{LNCS 6185 - Web-Age Information Management}},
url = {https://link.springer.com/content/pdf/10.1007/978-3-642-16720-1.pdf#page=53}
}
@article{Danz2017,
author = {Danz, M. and Gr{\"{a}}f, T. and Michel, C. and Miclaus, A},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Danz et al. - 2017 - Formulate Question.pdf:pdf},
pages = {1--4},
title = {{Formulate Question}},
year = {2017}
}
@article{Worsch2011,
author = {Worsch, Thomas},
file = {:Users/navolskyi/Google Drive/Studium/Bachelor/Informatik/Theoretische Informatik/1. Grudbegriffe der Informatik/Folien/Skript/skript-2013.pdf:pdf},
number = {November},
title = {{Grundbegriffe der Informatik}},
url = {https://physik.leech.it/pub/Info/Grundbegriffe_der_Informatik/Skripte/WS_10-11_Skript_Worsch.pdf},
year = {2011}
}
@article{Kasireddy2017,
author = {Kasireddy, Preethi},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Kasireddy - 2017 - A Beginner-Friendly Introduction to Containers , VMs and Docker What are “ containers ” and “ VMs ”.pdf:pdf},
pages = {1--16},
title = {{A Beginner-Friendly Introduction to Containers , VMs and Docker What are “ containers ” and “ VMs ”?}},
year = {2017}
}
@article{Guo2005a,
abstract = {We describe our method for benchmarking Semantic Web knowledge base systems with respect to use in large OWL applications. We present the Lehigh University Benchmark (LUBM) as an example of how to design such benchmarks. The LUBM features an ontology for the university domain, synthetic OWL data scalable to an arbitrary size, 14 extensional queries representing a variety of properties, and several performance metrics. The LUBM can be used to evaluate systems with different reasoning capabilities and storage mechanisms. We demonstrate this with an evaluation of two memory-based systems and two systems with persistent storage. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Guo, Yuanbo and Pan, Zhengxiang and Heflin, Jeff},
doi = {10.1016/j.websem.2005.06.005},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Guo, Pan, Heflin - 2005 - LUBM A benchmark for OWL knowledge base systems.pdf:pdf},
isbn = {1570-8268},
issn = {15708268},
journal = {Web Semant.},
keywords = {Evaluation,Knowledge base system,Lehigh University Benchmark,Semantic Web},
number = {2-3},
pages = {158--182},
title = {{LUBM: A benchmark for OWL knowledge base systems}},
url = {http://www.websemanticsjournal.org/index.php/ps/article/download/70/68},
volume = {3},
year = {2005}
}
@misc{OrientDB,
author = {{Orient Technologies}},
title = {{Getting Started {\textperiodcentered} OrientDB Manual}},
url = {https://orientdb.com/docs/last/Tutorial-Introduction-to-the-NoSQL-world.html},
urldate = {2018-04-29}
}
@article{Curry2012a,
abstract = {Within the operational phase buildings are now producing more data than ever before, from energy usage, utility information, occupancy patterns, weather data, etc. In order to manage a building holistically it is important to use knowledge from across these information sources. However, many barriers exist to their interoperability and there is little interaction between these islands of information. As part of moving building data to the cloud there is a critical need to reflect on the design of cloudbased data services and how they are designed from an interoperability perspective. If new cloud data services are designed in the same manner as traditional building management systems they will suffer from the data interoperability problems. Linked data technology leverages the existing open protocols and W3C standards of the Web architecture for sharing structured data on the web. In this paper we propose the use of linked data as an enabling technology for cloud-based building data services. The objective of linking building data in the cloud is to create an integrated well-connected graph of relevant information for managing a building. This paper describes the fundamentals of the approach and demonstrates the concept within a Small Medium sized Enterprise (SME) with an owner-occupied office building.},
author = {Curry, Edward and O'Donnell, James and Corry, Edward and Hasan, Souleiman and Keane, Marcus and O'Riain, Se{\'{a}}n},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Curry et al. - 2012 - Linking building data in the cloud Integrating cross-domain building data using linked data.pdf:pdf},
journal = {Elsevier Ltd.},
keywords = {Building energy analysis,Building infromation model,Building management,Data Interoperability,Data es a service,Linked data},
mendeley-tags = {Building energy analysis,Building infromation model,Building management,Data Interoperability,Data es a service,Linked data},
pages = {206--219},
title = {{Linking building data in the cloud: Integrating cross-domain building data using linked data}},
url = {http://s3.amazonaws.com/academia.edu.documents/41524058/Linking_building_data_in_the_cloud_Integ20160124-26435-1mfp5qv.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1499852371&Signature=wJPbGo7WnEgFytJCGI4kj3qeipk%253D&response-content-disposition=inlin},
year = {2012}
}
@article{Statement2017a,
author = {Statement, Problem and Idea, Basic},
file = {:Users/navolskyi/Library/Application Support/Mendeley Desktop/Downloaded/Statement, Idea - 2017 - Bachelor Thesis Shop-Floor Linked Data Graph - Design Of A Linked Data Processing System Architecture For The.pdf:pdf},
pages = {6--8},
title = {{Bachelor Thesis ?: Shop-Floor Linked Data Graph - Design Of A Linked Data Processing System Architecture For The Manufacturing Domain =}},
year = {2017}
}

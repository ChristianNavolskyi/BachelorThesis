\chapter{Evaluation}
\label{ch:evaluation}
This chapter will cover the execution and evaluation of our benchmark with the workloads specified in section~\ref{ch:design:se:workloads}.
We will present the results of each workload and have a discussion on them directly after that.

A conclusion will be drawn in section~\ref{ch:futureWork:se:conclusion} in the next chapter.

\section{Objective}
The main goal is to see,
if the databases are capable of handling the production workloads.
To test that feature we will also make some other performance benchmarks to be able to evaluate the write speeds of the databases.

We want to measure the average time needed for a single insert operation,
that way we can compare the databases without take into account the overhead of the benchmark itself,
which will be reflected in the overall run time.

For the workloads including read operations we will also look at the overall run time to get a better view on the impact these operations have on the performance.

In section~\ref{ch:evaluation:se:overview} we will show which workloads we will compare with each other and what we want to evaluate through that comparison.

\section{Setup}
In this section we will describe the software and hardware we used to execute the benchmark.

\subsection{Hardware}
The computer used for the benchmark had the specifications shown in table~\ref{tab:hardware}.

\begin{table}[!h]
  \begin{minipage}{\textwidth}
    \begin{tabularx}{\textwidth}{ | l | X | }
      \hline
      Component & Description \\ \hline \hline
      CPU & Intel i7-3770K @ 3.5GHz \\ \hline
      RAM & 16GB DDR3 @ 1.600MHz \\ \hline
      Storage & Seagate ST2000DL003 2 TB 5900rpm, only a 400GB partition was used \\ \hline
      GPU & NVIDIA GeForce GTX 670 \\ \hline
    \end{tabularx}
  \end{minipage}
  \caption{The hardware specifications of the computer for the benchmark.}
  \label{tab:hardware}
\end{table}

\subsection{Software}
The versions of the software components we used are shown in the following table.

\begin{table}[!h]
  \begin{minipage}{\textwidth}
    \begin{tabularx}{\textwidth}{ | X | X | }
      \hline
      Software & Version \\ \hline \hline
      Ubuntu & 17.10 \\ \hline
      Java & 1.8.0\_171 \\ \hline
      OpenSSH & 7.5p1 \\ \hline
      YCSB & 0.14.0-SNAPSHOT \\ \hline
      ApacheJena & 3.6.0 \\ \hline
      Neo4j & 3.3.4 \\ \hline
      OrientDB & 2.2.33 \\ \hline
      Sparksee & 5.2.3 \\ \hline
    \end{tabularx}
  \end{minipage}
  \caption{The software specifications of the computer for the benchmark.}
  \label{tab:software}
\end{table}

\section{Overview}
\label{ch:evaluation:se:overview}
In figure~\ref{fig:executionWorkflow} the execution process is illustrated.
From the set of defined workloads,
one is chosen and a dataset is created with the parameters specified for that workload.
The execute block takes that dataset,
the workload file,
a database from the set of databases and executes the workload on that database three times.
During each execution the measurements are saved to files in a folder specific to the constellation of workload,
database and the execution pass.
After one database has finished the third execution the next database is selected.
When all databases are finished with the workload the next workload is chosen and the cycle repeats itself until all workloads were executed.

\begin{figure}[!h]
  \includegraphics[width=\textwidth]{images/executionWorkflow}
  \caption{Workflow for the execution process.}
  \label{fig:executionWorkflow}
\end{figure}

In table~\ref{tab:throughputOverview} and~\ref{tab:productionOverview} the groups of workloads we are comparing with each other are shown.
The naming of the workloads is similar to the naming introduced in section~\ref{ch:design:se:workloads}.

\begin{landscape}
  \begin{table}
    \begin{minipage}{\hsize}
      \begin{tabularx}{\hsize}{ | l | l | l | l | X | }
        \hline
        Section & First workload & Other workload(s) & Units of measurement & Reason \\ \hline
        \ref{ch:evaluation:se:probingNodeCount} & 1. With Index & 2.-5. With Index & Inserts/second, total time, database size & The throughput in inserts/second will show if the databases slow down over time when they get filled up.
        The total time will show us,
        when the maximum dataset size is reached for each individual database in terms of reasonable execution time.\\ \hline
        \ref{ch:evaluation:se:probingNodeCount} & 1. Without Index & 2.-5. Without Index & Inserts/second & The throughput will show if the databases slow down as they get filled. \\ \hline
        \ref{ch:evaluation:se:probingNodeCount} & n.\footnote{the workload with the largest possible amount of nodes in terms of execution time.} With Index & n. Without Index & Inserts/second & To see how much time indexing takes up. \\ \hline
        \ref{ch:evaluation:se:probingNodeSize} & 1. Node Size & 2.-5. Node Size & Inserts/second, database size & We want to find the amount of data at which the databases are significantly slower.
        The database size of the different databases will show their storage efficiency. \\ \hline
        \ref{ch:evaluation:se:differenceEdges} & 1. No Edges & 2. No Edges & Inserts/second & Check if there is a benefit of an index if only nodes are inserted. \\ \hline
        \ref{ch:evaluation:se:differenceEdges} & n. With Index & 1. No Edges & Inserts/second & How much does inserting edges cost. \\ \hline
      \end{tabularx}
    \end{minipage}
    \caption{Overview for the throughput workloads \todo{remove reason and mention in evaluation}}
    \label{tab:throughputOverview}
  \end{table}
  \begin{table}
    \begin{minipage}{\hsize}
      \begin{tabularx}{\hsize}{ | l | l | l | l | X | }
        \hline
        Section & First workload & Other workload(s) & Units of measurement & Reason \\ \hline
        \ref{ch:evaluation:se:productComplexity} & 1. Structure & 2.-3. Structure & Inserts/second & Does the structure has an impact on performance. \\ \hline
        \ref{ch:evaluation:se:productionSuitability} & x.\footnote{Every workload will be evaluated} Suitablitiy & - & Total time & Check if the workload is completed faster then the production period it represents. \\ \hline
        \ref{ch:evaluation:se:retrievingUnderLoad} & 1. Reading & 2. Reading & Reads/second & Observe if there is a difference in using an index. \\ \hline
        \ref{ch:evaluation:se:retrievingUnderLoad} & 1. Scanning & 2. Scanning & Scans/second & See if there is a difference in using an index for scanning. \\ \hline
        \ref{ch:evaluation:se:retrievingUnderLoad} & 1. Structure & 1. Reading \& 1. Scanning & Operations/second & Investigate if other operations effect inserting data and compare operation throughput. \\ \hline
      \end{tabularx}
    \end{minipage}
    \caption{Overview for the production and retrieval workloads}
    \label{tab:productionOverview}
  \end{table}
\end{landscape}

After execution we have to combine the results for further inspection.
Figure~\ref{fig:evaluationWorkflow} illustrates this process of evaluation.
With all the results in one place we filtered the measurements for those we wanted,
then we calculated the average over the three benchmark runs we did with every database and workload.
Next we grouped the measurements as shown in tables~\ref{tab:throughputOverview} and~\ref{tab:productionOverview}.
Finally we created the diagrams shown in subsections "Results" of the following sections and interpreted them to draw a conclusion,
which is presented in the "Discussion" subsections.

\begin{figure}[!h]
  \includegraphics[width=\textwidth]{images/evaluationProcess}
  \caption{Workflow of the evaluation process.}
  \label{fig:evaluationWorkflow}
\end{figure}

\section{Throughput}
\label{ch:evaluation:se:throughput}
\todo{Tell at the beginning what we want to see. Insert includes edges, which have to look up nodes.}

In this section we will examine the combinations of workloads mentioned in table~\ref{tab:throughputOverview}.
The results from these workloads will give us an understanding of how the databases perform in terms of insertions per seconds depending on different factors.


\subsection{Probing Node Count}
\label{ch:evaluation:se:probingNodeCount}
Here we will compare how the throughput,
measured in inserts per second,
of the databases is effected by increasing the number of nodes we are inserting into it.
We will also look at the execution time,
to determine a reasonable large dataset in terms of execution time for the upcoming benchmark runs.

The throughput is listed in inserts per seconds,
which include both inserting nodes and inserting edges.
Note that in order to insert an edge the start and end node has to be looked up.

Apache Jena has no option to turn off indexing as mentioned in section~\ref{ch:background:se:apacheJena},
but it is still shown in the diagrams as reference.

\subsubsection{Results}
\todo{maybe include tables with numbers}
The first figure~\ref{fig:withIndexThroughput} shows how the different databases perform with an increasing dataset size.
Apache Jena and Neo4j only have values for 1.000 and 10.000 nodes,
because execution with more than 10.000 nodes would take too much time.
Sparksee only delivered results up to 100.000 nodes,
because the free license only included database sizes of up to 1.000.000 elements and a workload with 1.000.000 nodes would contain 2.333.333 elements in total with the edges.

In figure~\ref{fig:withIndexExecutionTime} we see the execution time of the different databases.
At 10.000 nodes Apache Jena and Neo4j took almost an hour for one run,
because of that we did not run it with 100.000 nodes or more.

\begin{figure}[h!]
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/throughput/withIndexThroughput}
    \captionof{figure}{This figure shows the throughput in inserts/second of every database over different dataset sizes.}
    \label{fig:withIndexThroughput}
  \end{minipage}
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/throughput/withIndexExecutionTime}
    \captionof{figure}{The execution time of the databases is shown over different dataset sizes.}
    \label{fig:withIndexExecutionTime}
  \end{minipage}
\end{figure}

Figure~\ref{fig:withoutIndexThroughput} shows the throughput over different dataset sizes without using an index.
In figure~\ref{fig:withWithoutIndexThroughputFixNodes} we see a comparison of using an index and not with a dataset size of 10.000 nodes.

\begin{figure}[h!]
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/throughput/withoutIndexThroughput}
    \captionof{figure}{This diagram shows the throughput in inserts per second while using no index.}
    \label{fig:withoutIndexThroughput}
  \end{minipage}
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/throughput/withWithoutIndexThroughputFixNodes}
    \captionof{figure}{The throughput at a fixed dataset size to compare between indexing and not.}
    \label{fig:withWithoutIndexThroughputFixNodes}
  \end{minipage}
\end{figure}

\subsubsection{Discussion}
Figure~\ref{fig:withWithoutIndexThroughputFixNodes} shows us,
that there is no performance change for Jena, Neo4j and OrientDB in using an index or not.
Sparksee shows a significant drop in throughput without the use of an index.
That is what we expected,
because the throughput also contains insertions of edges,
which have to look up nodes,
what is faster with an index.
For the other databases the lack of difference in performance might by,
that the benefit of using an index to retrieve the nodes for an edge is equalised by the time they take to insert the nodes into the index.

The execution time grows linearly,
which is a good sign,
because that means that the databases do scale for larger amounts of data.

If we compare the archived throughput with out target throughput of \todo{calculate target throughput},
we see that Sparksee exceeds our target with $ 16435 \frac{inserts}{s} $.
OrientDB misses our goal slightly,
at the larges dataset it only archived a throughput of $ 8572 \frac{inserts}{s} $.
Jena and Neo4j didn't even reach $ 10 \frac{inserts}{s} $.
These throughput values are measured with another data structure and node size than the one we will use for the suitability workload,
so we will investigate the factors differentiating this workload from the suitability workload and reference these results again in section~\ref{ch:evaluation:se:suitabilityDiscussion}.

From these results alone,
without looking at read performance separately we can say,
that an index is useful,
even for insert operations,
because edges need to look up two nodes,
which is faster when an index is used.

\subsection{Probing Node Size}
\label{ch:evaluation:se:probingNodeSize}
In this subsection we will take a look at how the databases perform with different node property sizes.
We will pick a dataset size of 10.000 nodes,
as all database have a reasonable execution time with that amount of nodes.

By investigating the performance under node size variation,
we will see if the databases can store larger amount of data in one node.
That can be useful depending on the use case,
in our example given by the industry only a two digit number is stored,
but it could be desirable to store longer number or more complex information.

\subsubsection{Results}
In figure~\ref{fig:nodeSize} we see,
how an increasing node size has an impact on insert throughput.

Sparksee only has values for node sizes up to $ 1KB $,
because the property we used to store the value of the node only supports up to 2048 characters/Bytes.

Figure~\ref{fig:sizeDatabaseSize} shows the size of the database folder,
in which the database stores its files.

\begin{figure}[h!]
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/throughput/nodeSize}
    \captionof{figure}{Insert throughput over different node sizes with 10.000 nodes total.}
    \label{fig:nodeSize}
  \end{minipage}
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/throughput/sizeDatabaseSize}
    \captionof{figure}{The size of the databases over growing node sizes.}
    \label{fig:sizeDatabaseSize}
  \end{minipage}
\end{figure}

\subsubsection{Discussion}
Figure~\ref{fig:nodeSize} show that the throughput of Jena and Neo4j is quite low and it doesn't show much difference with larger node sizes,
but at $ 1MB $ we can see that the performance decreases even more.

For OrientDB we se good performance up to $ 1KB $,
it starts to decline for node sizes of $ 10KB $ and above with a significant drop at $ 1MB $.

Sparksee has the highest throughput of all four databases,
but as it could only handle sized of up to $ 2KB $ or $ 1KB $ in our test scenario.
In that range the other databases also show no noteworthy change in performance,
so we can't draw a conclusion about the behaviour of Sparksee with larger node sizes.

In figure~\ref{fig:sizeDatabaseSize} we see that the database size grows linearly with the node size,
from $ 10KB $ and above.
So for smaller node values the overhead of the database itself determines the size of the database.

When we look closely at the values of Neo4j,
we can see that they are above the other database.
In fact at $ 1MB $ node size,
which would result in $ 10GB $ data for 10.000 nodes,
Neo4js database folder had a size of $ 22GB $,
so the overhead is more than the data itself.

\subsection{Difference without Edges}
\label{ch:evaluation:se:differenceEdges}
Here we will investigate how the absents of edges has an impact on performance.
These workloads to not represent a real world scenario,
but they will provide us knowledge about how much inserting nodes costs compared to edges,
as for every edge its start and end node have to be looked up.

Apache Jena always uses an index,
but it is still shown in both parts of the diagram as reference.

\subsubsection{Results}
Figure~\ref{fig:noEdges} shows us the difference in using an index compared to not doing so,
while only inserting nodes.

\begin{figure}[h!]
  \centering
  \includegraphics[width=.5\textwidth]{images/throughput/noEdges}
  \caption{Difference between using an index and not while using no edges.}
  \label{fig:noEdges}
\end{figure}

In figure~\ref{fig:indexNoEdges10000Nodes} we see a comparison of all databases between using edges and not with a dataset size of 10.000 nodes.
Figure~\ref{fig:indexNoEdges100000Nodes} shows a similar comparison with a bigger dataset,
but only between OrientDB and Sparksee,
as they were able to handle larger dataset within an acceptable time frame.

\begin{figure}[h!]
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/throughput/indexNoEdges10000Nodes}
    \captionof{figure}{Comparison of insert throughput between using edges and not.}
    \label{fig:indexNoEdges10000Nodes}
  \end{minipage}
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/throughput/indexNoEdges100000Nodes}
    \captionof{figure}{Comparison of the insert throughput with 100.000 nodes between using edges and not.}
    \label{fig:indexNoEdges100000Nodes}
  \end{minipage}
\end{figure}

\subsubsection{Discussion}
\todo{Do edges effect throughput}
As expected in figure~\ref{fig:noEdges} we can see,
that using an index does not benefit the insert performance for nodes.
Only Neo4j shows a notable difference for the node insert throughput with the use of an index the throughput drop to $ 6,6 \frac{inserts}{s} $ from $ 12 \frac{inserts}{s} $ without an index.

Figure~\ref{fig:indexNoEdges10000Nodes} shows,
that the insert throughput of Apache Jena is slightly lower when using edges.

\section{Production Simulation}
\label{ch:evaluation:se:productionSimulation}
The workload results presented in this section will cover the production specific variables.
The first one being product complexity and the other one execution time.

\subsection{Product Complexity}
\label{ch:evaluation:se:productComplexity}
\todo{From Design: Number of edges could effect the throughput, because of inserts. social networks have much more edges to it than our structure}
If the throughput is not effected by using no edges compared to using edges,
we could see,
that for a graph database the structure of the data has no impact on performance.
This result is quite important,
as it would lead to the conclusion,
that we can use the results of other graph database benchmarks for our industrial use case.

The product complexity describes,
how much the tree representing our data structure is widened at three different levels shown in section~\ref{ch:design:se:dataStructure}.

The wider the data structure becomes the less edges we have per node.
That can be interesting if we want to compare the generalisation of the throughput with different graph structures,
which we need to determine if the throughput archived in section~\ref{ch:evaluation:se:probingNodeCount} can be used to draw conclusions about the suitability for the industrial environment.

\subsubsection{Results}
In figure~\ref{fig:structure} we see the impact a different data structure has on the insert throughput.

\begin{figure}[h!]
  \centering
  \includegraphics[width=.5\textwidth]{images/production/structure}
  \caption{Shows the difference in insert throughput over changing data structure.}
  \label{fig:structure}
\end{figure}

\subsubsection{Discussion}
As we see in figure~\ref{fig:structure} the structure of the data as we modelled it,
doesn't effect the throughput of the databases.

For the "simple" workload the have an edge to node ratio of \todo{calculate} which is dropping to \todo{calculate} for the "most complex" workload.
We cannot draw a conclusion about the comparability with other related work using social network graphs,
which have a much higher edge to node ratio,
in order to do so we would have to investigate higher edge to node ratios.

\subsection{Production Suitability}
\label{ch:evaluation:se:productionSuitability}
The production simulations will finally show,
if the databases we chose are capable of storing the necessary amount of data in a specified time interval.

In the discussion of this section we will also investigate the throughput based on the previous workloads.

\subsubsection{Results}
Figure~\ref{fig:singleSuitability} show how long OrientDB took,
to store three minutes of production data (1.056.833 nodes).
Sparksee is mentioned with a theoretical time,
since it only allowed us to store 500.000 elements.
We took the throughput during inserting these 500.000 elements and calculated the time it would need to complete the whole workload.
The same was done for the results shown in figure~\ref{fig:hourSuitability}.

In figure~\ref{fig:hourSuitability} the same is shown but with a dataset that represents one hour of production,
which contains 21.136.660 nodes.

Only OrientDB and Sparksee were used in these workloads,
because Apache Jena and Neo4j would take too long to insert that amount of nodes.

\begin{figure}[h!]
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/production/singleSuitability}
    \captionof{figure}{Shows the execution time with a dataset that represents three minutes of production.}
    \label{fig:singleSuitability}
  \end{minipage}
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/production/hourSuitability}
    \captionof{figure}{Shows the execution time with a dataset that represents one hour of production.}
    \label{fig:hourSuitability}
  \end{minipage}
\end{figure}

\subsubsection{Discussion}
\label{ch:evaluation:se:suitabilityDiscussion}
The figures~\ref{fig:singleSuitability} and~\ref{fig:hourSuitability} show us,
that OrientDB did not manage to store the three minutes or the one hour of production simulation in the specified time.

Sparksee could theoretically store that amount without exceeding the time limit,
but since the free license did not allow for the amount of data we used the average util the limit was reached.
Of course it could be,
that the throughput of Sparksee drops with an increasing number of elements in the database,
but we couldn't investigate that.

The difference of this workload compared to the first workload we discussed in section~\ref{ch:evaluation:se:probingNodeCount} is the structure and the node size.
The results of~\ref{ch:evaluation:se:probingNodeSize} and~\ref{ch:evaluation:se:productComplexity} show,
that the structure has no impact on the throughput and the node size has no impact below $ 10KB $,
since we used $ 50B $ we can compare the first measured throughput.

By doing so we see that Sparksee,
again theoretically,
reaches our target throughput of \todo{calculate}.
OrientDB misses our target with $ 8572 \frac{inserts}{s} $.
These results support our findings for the impact of structure and node size,
as this workload,
measuring the time correlates with the numbers from the first workload.

\section{Retrieving under load}
\label{ch:evaluation:se:retrievingUnderLoad}
\todo{compare to results of other studies}
This section will cover the results about retrieving data while the database is under load.
First we will take a look at how using an index is effecting the read and scan throughput,
then we will compare the throughput of the different operations~(\ref{fig:operationReadScan}) and their impact on the insert operation~(\ref{fig:insertWithWithoutReadScan}).

\subsection{Results}
In figure~\ref{fig:readThroughput10000Nodes} and~\ref{fig:scanThroughput10000Nodes} we see the throughput of read and scan operations when using an index or not.

\begin{figure}[h!]
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/responsiveness/readThroughput10000Nodes}
    \captionof{figure}{Shows the throughput of read operations with and without the use of an index.}
    \label{fig:readThroughput10000Nodes}
  \end{minipage}
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/responsiveness/scanThroughput10000Nodes}
    \captionof{figure}{Shows the throughput of read operations with and without the use of an index.}
    \label{fig:scanThroughput10000Nodes}
  \end{minipage}
\end{figure}

Figure~\ref{fig:operationReadScan} shows the throughput of the different operations.
In figure~\ref{fig:insertWithWithoutReadScan} we see the impact of the read and scan operations on the insert operations.

\begin{figure}[h!]
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/responsiveness/operationReadScan}
    \captionof{figure}{Shows the throughput of the different operations.\newline
    }
    \label{fig:operationReadScan}
  \end{minipage}
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/responsiveness/insertWithWithoutReadScan}
    \captionof{figure}{Shows the throughput of insert operations when using different operations.}
    \label{fig:insertWithWithoutReadScan}
  \end{minipage}
\end{figure}

\subsection{Discussion}
First we will discuss the results regarding the use of an index and not.
For read operations figure~\ref{fig:readThroughput10000Nodes} shows,
that all databases benefit from using an index,
except Apache Jena which always uses an index and is only presented as reference.
Sparksee shows the biggest difference in throughput of read operations,
whereas the others only show a slight decrease in performance without an index.
That was what we would expect,
since an index really benefits these kinds of operations,
although we expected the increase with the use of an index to be higher for Neo4j and OrientDB.
It could be,
that the dataset size is too small to show an effect between using an index and not.

Similar results can be seen for the scan operations shown in figure~\ref{fig:scanThroughput10000Nodes}.
The absents of an index has not much effect here either,
that could be the case,
because scan operations only use one read operations for the start node and then traverse the graph,
which is not effected by the index.

The comparison of the different operations shown in figure~\ref{fig:operationReadScan} shows us where the strengths and weaknesses are in the different databases.
Apache Jena and Neo4j are the slowest when it comes to inserting nodes,
but they are much faster in retrieving nodes,
with Neo4j even being the fastest of all four in graph traversal.

OrientDB and Sparksee seem to be a good choice then inserting and reading is the main concern of the application.

When we compare the results of Apache Jena from its read performance to its scan performance,
we see almost no difference in performance,
which means it is even faster than Neo4j in graph traversal,
but it is limited by the relatively slow read operation at the beginning of the scan operation.

The last figure~\ref{fig:insertWithWithoutReadScan} shows us,
that using other operations on the database does effect insert throughput,
except for Sparksee,
which seems to stay stable in its throughput even when other operations are being used.

Jena and Neo4j are low in throughput anyways,
but they still suffer from other operations being executed regularly.
OrientDB has a slightly worse throughput when using read operations and even worse with scan operations,
that is not good for our industry scenario,
because in the industry read and scan operations will be used on the database more or less regularly,
depending on the specific use case.
